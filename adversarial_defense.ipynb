{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "defense.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bfb8ac94683a41a28b4630859e6514d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a9ad3c15ec91405e90f379a256add768",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_71d828ec4cc74eed8c78109b6f3d7c27",
              "IPY_MODEL_8227fdc0ace5455eba4826e7dc707a66"
            ]
          }
        },
        "a9ad3c15ec91405e90f379a256add768": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "71d828ec4cc74eed8c78109b6f3d7c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4b10115c1275483bbe4031cbb8a8ca41",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3976f2ee15704913be7fb23bc847d11b"
          }
        },
        "8227fdc0ace5455eba4826e7dc707a66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4d5d82028eee4a55a865306773aafc1b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:19&lt;00:00, 30491720.11it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3c92813972f84e1cb1336c913a1aaf61"
          }
        },
        "4b10115c1275483bbe4031cbb8a8ca41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3976f2ee15704913be7fb23bc847d11b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d5d82028eee4a55a865306773aafc1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3c92813972f84e1cb1336c913a1aaf61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "regpg0pk45dM",
        "outputId": "435bcf6e-0b04-407f-8550-402b3d088902",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8_VO9K36G2Q"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrR1jH7N6Gl0"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import argparse\n",
        "# import apex.amp as amp\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "path = \"/content/gdrive/My Drive/GameTheory/\"\n",
        "\n",
        "\n",
        "gpu = True\n",
        "gpu = gpu and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if gpu else \"cpu\")\n",
        "\n",
        "'''hparameters'''\n",
        "# BATCH_SIZE = 200 \n",
        "# NUM_WORKERS = 8 \n",
        "# NUM_EPOCHS = 20 \n",
        "\n",
        "# numEpochs = 20\n",
        "num_feats = 3 # in_channels\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "learningRate = 0.01\n",
        "weightDecay = 5e-4\n",
        "\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqCWUSpIUrbP",
        "outputId": "7fe749ae-914d-4a36-c500-5c9e8395e478",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN9CTmoq53uy"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "201UIuYK52wN",
        "outputId": "801136ee-21fa-4355-f91d-f634602a40a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "bfb8ac94683a41a28b4630859e6514d2",
            "a9ad3c15ec91405e90f379a256add768",
            "71d828ec4cc74eed8c78109b6f3d7c27",
            "8227fdc0ace5455eba4826e7dc707a66",
            "4b10115c1275483bbe4031cbb8a8ca41",
            "3976f2ee15704913be7fb23bc847d11b",
            "4d5d82028eee4a55a865306773aafc1b",
            "3c92813972f84e1cb1336c913a1aaf61"
          ]
        }
      },
      "source": [
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfb8ac94683a41a28b4630859e6514d2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZZazL767hRl"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBpqIyLzMJf3"
      },
      "source": [
        "## MyResnet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYIe2rMw7hq3"
      },
      "source": [
        "# kernel_size=3, padding=1\n",
        "def conv3x3(in_channels, out_channels, stride=1):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
        "                     stride=stride, padding=1, bias=False)\n",
        "    \n",
        "# kernel_size=1, padding=0\n",
        "def conv1x1(in_channels, out_channels, stride=1):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=1, \n",
        "                     stride=stride, bias=False)\n",
        "\n",
        "num_classes = 10\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, inchannel, outchannel, stride=1, isDownSample=False):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        \n",
        "        self.inchannel = inchannel\n",
        "        self.expansion = 4\n",
        "        self.isDownSample = isDownSample\n",
        "        \n",
        "        self.conv1 = conv1x1(inchannel, outchannel)\n",
        "        self.norm1 = nn.BatchNorm2d(outchannel)\n",
        "\n",
        "        self.conv2 = conv3x3(outchannel, outchannel, stride)\n",
        "        self.norm2 = nn.BatchNorm2d(outchannel)\n",
        "        \n",
        "        self.conv3 = conv1x1(outchannel, outchannel * self.expansion)\n",
        "        self.norm3 = nn.BatchNorm2d(outchannel * self.expansion)\n",
        "        \n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "        if isDownSample:\n",
        "            self.downsample = nn.Sequential(\n",
        "                conv1x1(inchannel, outchannel * self.expansion, stride),\n",
        "                nn.BatchNorm2d(outchannel * self.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.relu(self.norm1(self.conv1(x)))\n",
        "        out = self.relu(self.norm2(self.conv2(out)))\n",
        "        out = self.relu(self.norm3(self.conv3(out)))\n",
        "\n",
        "        if self.isDownSample:\n",
        "            out += self.downsample(identity)\n",
        "        \n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet50(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=num_classes, outs=[64, 128, 256, 512]):\n",
        "        super(ResNet50, self).__init__()\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            block (class): BasicBlock(nn.Module)\n",
        "            layers (list): A ResNetâ€™s layer is composed of the same blocks stacked one after the other.\n",
        "            num_classes (int): num_classes = 4000\n",
        "            outs (list): dim before expension(*4)\n",
        "        \"\"\"\n",
        "        self.expansion = 4\n",
        "        self.inchannel = 64*self.expansion\n",
        "        self.conv0 = conv3x3(3, 64*self.expansion, stride=1)\n",
        "        \n",
        "        self.layer1=self.make_layer(block,outs[0],layers[0],stride=1) # 3\n",
        "        self.layer2=self.make_layer(block,outs[1],layers[1],stride=2) # 4\n",
        "        self.layer3=self.make_layer(block,outs[2],layers[2],stride=2) # 6\n",
        "        self.layer4=self.make_layer(block,outs[3],layers[3],stride=2) # 3\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512*4, num_classes)\n",
        "        \n",
        "        # self.cfc = nn.Linear(512*4, outFeat)\n",
        "        # self.crelu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def make_layer(self, block, out_channels, block_num, stride=1):\n",
        "        \"\"\"\n",
        "            block (class): BottleneckBlock(nn.Module)\n",
        "            out_channels (int)ï¼šoutput size of layer\n",
        "            block_num (int)ï¼štotal blocks\n",
        "            stride (int)ï¼šConv Block stride\n",
        "        \"\"\"\n",
        "\n",
        "        if stride!=1 or self.inchannel!=(out_channels*self.expansion):\n",
        "            isDownsample = True\n",
        "        else: isDownsample = False\n",
        "            \n",
        "        layers = []\n",
        "        #Conv Block: different size\n",
        "        conv_block=block(self.inchannel, out_channels, stride, isDownsample)\n",
        "        layers.append(conv_block)\n",
        "        self.inchannel = out_channels*self.expansion\n",
        "        \n",
        "        #Identity Block: same size\n",
        "        for i in range(1, block_num):\n",
        "            layers.append(block(self.inchannel, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, ver=False):\n",
        "        out = x\n",
        "        out = self.conv0(out)\n",
        "\n",
        "        out=self.layer1(out)\n",
        "        out=self.layer2(out)\n",
        "        out=self.layer3(out)\n",
        "        out=self.layer4(out)\n",
        "\n",
        "        out = self.avgpool(out)\n",
        "        # out = torch.squeeze(out)\n",
        "        out = out.reshape(out.shape[0], out.shape[1])\n",
        "        \n",
        "        # embed = out\n",
        "        out = self.fc(out)\n",
        "        # cout = self.cfc(out)\n",
        "        # cout = self.crelu(cout)\n",
        "        return out\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_normal_(m.weight.data)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aztOJFmMYzf"
      },
      "source": [
        "## BasicBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zrpU5m5Ml1H"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FAee3LfMnXZ"
      },
      "source": [
        "## BottleNeck"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7IFf7F-Mr1z"
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDgXKfVEMVFl"
      },
      "source": [
        "## ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1uBMTUfMTX-"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STAvhnJkM8R0"
      },
      "source": [
        "def init_weights(m):\n",
        "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_normal_(m.weight.data)\n",
        "        \n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErvZeo4KIURN"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyccGfvzIQ7L"
      },
      "source": [
        "def train(net, epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "        del inputs\n",
        "        del targets\n",
        "\n",
        "        # progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        #              % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    acc = correct/total\n",
        "    avg_loss = train_loss/total\n",
        "    \n",
        "    return avg_loss, acc\n",
        "\n",
        "\n",
        "def test(net, epoch):\n",
        "    # global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            \n",
        "            torch.cuda.empty_cache()\n",
        "            del inputs\n",
        "            del targets\n",
        "            \n",
        "            # progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "            #              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    acc = correct/total\n",
        "    avg_loss = test_loss/total\n",
        "    \n",
        "    return avg_loss, acc\n",
        "    # Save checkpoint.\n",
        "    # acc = 100.*correct/total\n",
        "    # if acc > best_acc:\n",
        "    #     print('Saving..')\n",
        "    #     state = {\n",
        "    #         'net': net.state_dict(),\n",
        "    #         'acc': acc,\n",
        "    #         'epoch': epoch,\n",
        "    #     }\n",
        "    #     if not os.path.isdir('checkpoint'):\n",
        "    #         os.mkdir('checkpoint')\n",
        "    #     torch.save(state, './checkpoint/ckpt.pth')\n",
        "    #     best_acc = acc"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q266TLckp0Oy"
      },
      "source": [
        "# Adversarial training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5djSpMsBJHPR"
      },
      "source": [
        "## FGSM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZwMx2qlp7Xl"
      },
      "source": [
        "def fgsm_train(net, epoch, eps=0.01):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        inputs.requires_grad = True\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        data_grad = inputs.grad.data\n",
        "        perturbed_data = fgsm_attack(inputs, eps, data_grad)\n",
        "        new_outputs = net(perturbed_data)\n",
        "        new_loss = criterion(new_outputs, targets)\n",
        "        new_loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += new_loss.item()\n",
        "        _, new_predicted = new_outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += new_predicted.eq(targets).sum().item()\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "        del inputs\n",
        "        del targets\n",
        "\n",
        "        # progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        #              % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    acc = correct/total\n",
        "    avg_loss = train_loss/total\n",
        "    \n",
        "    return avg_loss, acc"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2JqVoITJBJ-"
      },
      "source": [
        "## PGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u25OQlaFL081"
      },
      "source": [
        "def clamp(X, lower_limit, upper_limit):\n",
        "    return torch.max(torch.min(X, upper_limit), lower_limit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpbxy4wbQsZ7"
      },
      "source": [
        "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar10_std = (0.2471, 0.2435, 0.2616)\n",
        "\n",
        "mu = torch.tensor(cifar10_mean).view(3,1,1).cuda()\n",
        "std = torch.tensor(cifar10_std).view(3,1,1).cuda()\n",
        "\n",
        "upper_limit = ((1 - mu) / std)\n",
        "lower_limit = ((0 - mu) / std)\n",
        "\n",
        "epsilon = (8/255.) / std\n",
        "\n",
        "step_size = 2\n",
        "\n",
        "iters = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4G_ji-HI_yv"
      },
      "source": [
        "# Training\n",
        "def pgd_train(model, epoch):\n",
        "    # start_train_time = time.time()\n",
        "\n",
        "    # logger.info('Epoch \\t Seconds \\t LR \\t \\t Train Loss \\t Train Acc')\n",
        "\n",
        "# for epoch in range(args.epochs):\n",
        "# for epoch in range(start_epoch, start_epoch+20):\n",
        "    start_epoch_time = time.time()\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    train_n = 0\n",
        "\n",
        "    for i, (X, y) in enumerate(trainloader):\n",
        "        X, y = X.cuda(), y.cuda()\n",
        "        delta = torch.zeros_like(X).cuda()\n",
        "\n",
        "        # if args.delta_init == 'random':\n",
        "        for i in range(len(epsilon)):\n",
        "            delta[:, i, :, :].uniform_(-epsilon[i][0][0].item(), epsilon[i][0][0].item())\n",
        "        delta.data = clamp(delta, lower_limit-X, upper_limit-X)\n",
        "\n",
        "        delta.requires_grad = True\n",
        "        for _ in range(iters):\n",
        "            output = model(X + delta)\n",
        "            loss = criterion(output, y)\n",
        "\n",
        "            # with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "            loss.backward()\n",
        "            \n",
        "            grad = delta.grad.detach()\n",
        "            delta.data = clamp(delta + step_size*torch.sign(grad), -epsilon, epsilon)\n",
        "            delta.data = clamp(delta, lower_limit-X, upper_limit-X)\n",
        "            delta.grad.zero_()\n",
        "        \n",
        "        delta = delta.detach()\n",
        "        output = model(X + delta)\n",
        "        loss = criterion(output, y)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        # with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item() * y.size(0)\n",
        "        train_acc += (output.max(1)[1] == y).sum().item()\n",
        "        train_n += y.size(0)\n",
        "        \n",
        "        # scheduler.step()\n",
        "    epoch_time = time.time()\n",
        "    # lr = scheduler.get_lr()[0]\n",
        "    # logger.info('%d \\t %.1f \\t \\t %.4f \\t %.4f \\t %.4f',\n",
        "    #             epoch, epoch_time-start_epoch_time, train_loss/train_n, train_acc/train_n)\n",
        "    \n",
        "    print(epoch_time-start_epoch_time)\n",
        "    return train_loss/train_n, train_acc/train_n\n",
        "# train_time = time.time()\n",
        "\n",
        "# torch.save(model.state_dict(), os.path.join(args.out_dir, 'model.pth'))\n",
        "\n",
        "# logger.info('Total train time: %.4f minutes', (train_time - start_train_time)/60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VTg86X3fxfd"
      },
      "source": [
        "# Train the model\n",
        "# optimizer = torch.optim.RMSprop(net.parameters(), lr=param['learning_rate'],\n",
        "def adv_train2(net, epoch, loader_train):\n",
        "\n",
        "# for epoch in range(epoch):\n",
        "    print('Starting epoch %d / %d' % (epoch + 1, epoch))\n",
        "\n",
        "    for t, (x, y) in enumerate(loader_train):\n",
        "\n",
        "        x_var, y_var = to_var(x), to_var(y.long())\n",
        "        loss = criterion(net(x_var), y_var)\n",
        "\n",
        "        # adversarial training\n",
        "        if epoch+1 > param['delay']:\n",
        "            # use predicted label to prevent label leaking\n",
        "            y_pred = pred_batch(x, net)\n",
        "            x_adv = adv_train(x, y_pred, net, criterion, adversary)\n",
        "            x_adv_var = to_var(x_adv)\n",
        "            loss_adv = criterion(net(x_adv_var), y_var)\n",
        "            loss = (loss + loss_adv) / 2\n",
        "\n",
        "        if (t + 1) % 100 == 0:\n",
        "            print('t = %d, loss = %.8f' % (t + 1, loss.data[0]))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgugvL2vpIEQ"
      },
      "source": [
        "# Adversarial Attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXAXVIztJNje"
      },
      "source": [
        "## FGSM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xelOTRJKFXPx"
      },
      "source": [
        "def testattack(net, testloader, epsilon):\n",
        "    # global best_acc\n",
        "    net.eval()\n",
        "\n",
        "    # test_loss = 0\n",
        "    correct = 0\n",
        "    # total = 0\n",
        "    # with torch.no_grad():\n",
        "    for b, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        inputs.requires_grad = True\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        # if predicted.item() != targets.item():\n",
        "        #     continue\n",
        "        loss = criterion(outputs, targets)\n",
        "        net.zero_grad()\n",
        "\n",
        "        # test_loss += loss.item()\n",
        "        loss.backward()\n",
        "        data_grad = inputs.grad.data\n",
        "        perturbed_data = fgsm_attack(inputs, epsilon, data_grad)\n",
        "        new_outputs = net(perturbed_data)\n",
        "        _, new_predicted = new_outputs.max(1)\n",
        "\n",
        "        # total += targets.size(0)\n",
        "        correct += new_predicted.eq(targets).sum().item()\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        del inputs\n",
        "        del targets\n",
        "        \n",
        "    acc = correct/float(len(testloader))\n",
        "    # avg_loss = test_loss/total\n",
        "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(testloader), acc))\n",
        "\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lPJi6dtpKaI"
      },
      "source": [
        "def attack(model, test_loader, epsilon):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    adv_examples = []\n",
        "\n",
        "    # Loop over all examples in test set\n",
        "    for data, target in test_loader:\n",
        "\n",
        "        # Send the data and label to the device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Set requires_grad attribute of tensor. Important for Attack\n",
        "        data.requires_grad = True\n",
        "\n",
        "        # Forward pass the data through the model\n",
        "        output = model(data)\n",
        "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        # _, predicted = output.max(1)\n",
        "\n",
        "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
        "        if init_pred.item() != target.item():\n",
        "            continue\n",
        "\n",
        "        # Calculate the loss\n",
        "        # loss = F.nll_loss(output, target)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Zero all existing gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Calculate gradients of model in backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Collect datagrad\n",
        "        data_grad = data.grad.data\n",
        "\n",
        "        # Call FGSM Attack\n",
        "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
        "\n",
        "        # Re-classify the perturbed image\n",
        "        output = model(perturbed_data)\n",
        "\n",
        "        # Check for success\n",
        "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        if final_pred.item() == target.item():\n",
        "            correct += 1\n",
        "            # Special case for saving 0 epsilon examples\n",
        "            if (epsilon == 0) and (len(adv_examples) < 5):\n",
        "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
        "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
        "        else:\n",
        "            # Save some adv examples for visualization later\n",
        "            if len(adv_examples) < 5:\n",
        "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
        "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        del data\n",
        "        del target\n",
        "\n",
        "    # Calculate final accuracy for this epsilon\n",
        "    final_acc = correct/float(len(test_loader))\n",
        "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
        "\n",
        "    # Return the accuracy and an adversarial example\n",
        "    return final_acc, adv_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QyAk08dJYMr"
      },
      "source": [
        "## PGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfsZPI-LDhV8"
      },
      "source": [
        "def testattack2(model, testloader):\n",
        "    print(\"Attack Image & Predicted Label\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in testloader:\n",
        "        \n",
        "        images = pgd_attack(model, images, labels)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        \n",
        "        # imshow(torchvision.utils.make_grid(images.cpu().data, normalize=True), [normal_data.classes[i] for i in pre])\n",
        "        \n",
        "    final_acc = float(correct)/total\n",
        "    print('Accuracy of test text: %f %%' % (100 * float(correct)/total))\n",
        "\n",
        "    return final_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fITSiBlAIgAM"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVvfCFNYIhb9"
      },
      "source": [
        "\"\"\"ResNet50 Model\"\"\"\n",
        "# resmodel = ResNet50(Bottleneck, [3,4,6,3]).to(device)\n",
        "resmodel = ResNet50().to(device)\n",
        "resmodel.apply(init_weights)\n",
        "\n",
        "if device == 'cuda':\n",
        "    resmodel = torch.nn.DataParallel(resmodel)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(resmodel.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "# optimizer = torch.optim.Adam(resmodel.parameters(), lr=1e-3)\n",
        "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-UXfgypT8zI"
      },
      "source": [
        "## free train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiowZXx5KPr5",
        "outputId": "94d52fef-9ed6-4a9f-9fce-29aed6298f31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(start_epoch, start_epoch+50):\n",
        "    train_loss, train_acc = train(resmodel, epoch)\n",
        "    val_loss, val_acc = test(resmodel, epoch)\n",
        "    \n",
        "    print('[Epoch: {}]\\nTrain Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
        "          format(epoch, train_loss, train_acc, val_loss, val_acc))\n",
        "    \n",
        "    if (epoch+1)%10 == 0:\n",
        "        torch.save({'model_state_dict': resmodel.state_dict(),},\n",
        "                    path + \"ResNet50_{}.pth\".format(str(epoch)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "[Epoch: 0]\n",
            "Train Loss: 0.0034\tTrain Accuracy: 0.8527\tVal Loss: 0.0052\tVal Accuracy: 0.8247\n",
            "\n",
            "Epoch: 1\n",
            "[Epoch: 1]\n",
            "Train Loss: 0.0031\tTrain Accuracy: 0.8616\tVal Loss: 0.0058\tVal Accuracy: 0.8096\n",
            "\n",
            "Epoch: 2\n",
            "[Epoch: 2]\n",
            "Train Loss: 0.0029\tTrain Accuracy: 0.8715\tVal Loss: 0.0047\tVal Accuracy: 0.8422\n",
            "\n",
            "Epoch: 3\n",
            "[Epoch: 3]\n",
            "Train Loss: 0.0027\tTrain Accuracy: 0.8816\tVal Loss: 0.0050\tVal Accuracy: 0.8370\n",
            "\n",
            "Epoch: 4\n",
            "[Epoch: 4]\n",
            "Train Loss: 0.0025\tTrain Accuracy: 0.8867\tVal Loss: 0.0051\tVal Accuracy: 0.8370\n",
            "\n",
            "Epoch: 5\n",
            "[Epoch: 5]\n",
            "Train Loss: 0.0024\tTrain Accuracy: 0.8933\tVal Loss: 0.0049\tVal Accuracy: 0.8442\n",
            "\n",
            "Epoch: 6\n",
            "[Epoch: 6]\n",
            "Train Loss: 0.0022\tTrain Accuracy: 0.9012\tVal Loss: 0.0045\tVal Accuracy: 0.8586\n",
            "\n",
            "Epoch: 7\n",
            "[Epoch: 7]\n",
            "Train Loss: 0.0021\tTrain Accuracy: 0.9076\tVal Loss: 0.0044\tVal Accuracy: 0.8590\n",
            "\n",
            "Epoch: 8\n",
            "[Epoch: 8]\n",
            "Train Loss: 0.0020\tTrain Accuracy: 0.9092\tVal Loss: 0.0042\tVal Accuracy: 0.8685\n",
            "\n",
            "Epoch: 9\n",
            "[Epoch: 9]\n",
            "Train Loss: 0.0019\tTrain Accuracy: 0.9155\tVal Loss: 0.0046\tVal Accuracy: 0.8597\n",
            "\n",
            "Epoch: 10\n",
            "[Epoch: 10]\n",
            "Train Loss: 0.0018\tTrain Accuracy: 0.9188\tVal Loss: 0.0040\tVal Accuracy: 0.8732\n",
            "\n",
            "Epoch: 11\n",
            "[Epoch: 11]\n",
            "Train Loss: 0.0017\tTrain Accuracy: 0.9243\tVal Loss: 0.0044\tVal Accuracy: 0.8696\n",
            "\n",
            "Epoch: 12\n",
            "[Epoch: 12]\n",
            "Train Loss: 0.0016\tTrain Accuracy: 0.9274\tVal Loss: 0.0038\tVal Accuracy: 0.8810\n",
            "\n",
            "Epoch: 13\n",
            "[Epoch: 13]\n",
            "Train Loss: 0.0015\tTrain Accuracy: 0.9312\tVal Loss: 0.0042\tVal Accuracy: 0.8717\n",
            "\n",
            "Epoch: 14\n",
            "[Epoch: 14]\n",
            "Train Loss: 0.0015\tTrain Accuracy: 0.9346\tVal Loss: 0.0050\tVal Accuracy: 0.8625\n",
            "\n",
            "Epoch: 15\n",
            "[Epoch: 15]\n",
            "Train Loss: 0.0014\tTrain Accuracy: 0.9359\tVal Loss: 0.0047\tVal Accuracy: 0.8625\n",
            "\n",
            "Epoch: 16\n",
            "[Epoch: 16]\n",
            "Train Loss: 0.0014\tTrain Accuracy: 0.9389\tVal Loss: 0.0039\tVal Accuracy: 0.8838\n",
            "\n",
            "Epoch: 17\n",
            "[Epoch: 17]\n",
            "Train Loss: 0.0013\tTrain Accuracy: 0.9404\tVal Loss: 0.0040\tVal Accuracy: 0.8828\n",
            "\n",
            "Epoch: 18\n",
            "[Epoch: 18]\n",
            "Train Loss: 0.0013\tTrain Accuracy: 0.9439\tVal Loss: 0.0042\tVal Accuracy: 0.8807\n",
            "\n",
            "Epoch: 19\n",
            "[Epoch: 19]\n",
            "Train Loss: 0.0012\tTrain Accuracy: 0.9475\tVal Loss: 0.0037\tVal Accuracy: 0.8939\n",
            "\n",
            "Epoch: 20\n",
            "[Epoch: 20]\n",
            "Train Loss: 0.0012\tTrain Accuracy: 0.9476\tVal Loss: 0.0037\tVal Accuracy: 0.8918\n",
            "\n",
            "Epoch: 21\n",
            "[Epoch: 21]\n",
            "Train Loss: 0.0011\tTrain Accuracy: 0.9495\tVal Loss: 0.0046\tVal Accuracy: 0.8796\n",
            "\n",
            "Epoch: 22\n",
            "[Epoch: 22]\n",
            "Train Loss: 0.0011\tTrain Accuracy: 0.9490\tVal Loss: 0.0036\tVal Accuracy: 0.8931\n",
            "\n",
            "Epoch: 23\n",
            "[Epoch: 23]\n",
            "Train Loss: 0.0011\tTrain Accuracy: 0.9498\tVal Loss: 0.0043\tVal Accuracy: 0.8820\n",
            "\n",
            "Epoch: 24\n",
            "[Epoch: 24]\n",
            "Train Loss: 0.0010\tTrain Accuracy: 0.9532\tVal Loss: 0.0044\tVal Accuracy: 0.8795\n",
            "\n",
            "Epoch: 25\n",
            "[Epoch: 25]\n",
            "Train Loss: 0.0011\tTrain Accuracy: 0.9528\tVal Loss: 0.0037\tVal Accuracy: 0.8961\n",
            "\n",
            "Epoch: 26\n",
            "[Epoch: 26]\n",
            "Train Loss: 0.0010\tTrain Accuracy: 0.9569\tVal Loss: 0.0038\tVal Accuracy: 0.8922\n",
            "\n",
            "Epoch: 27\n",
            "[Epoch: 27]\n",
            "Train Loss: 0.0009\tTrain Accuracy: 0.9585\tVal Loss: 0.0040\tVal Accuracy: 0.8912\n",
            "\n",
            "Epoch: 28\n",
            "[Epoch: 28]\n",
            "Train Loss: 0.0009\tTrain Accuracy: 0.9581\tVal Loss: 0.0037\tVal Accuracy: 0.9013\n",
            "\n",
            "Epoch: 29\n",
            "[Epoch: 29]\n",
            "Train Loss: 0.0009\tTrain Accuracy: 0.9599\tVal Loss: 0.0043\tVal Accuracy: 0.8826\n",
            "\n",
            "Epoch: 30\n",
            "[Epoch: 30]\n",
            "Train Loss: 0.0009\tTrain Accuracy: 0.9583\tVal Loss: 0.0040\tVal Accuracy: 0.8952\n",
            "\n",
            "Epoch: 31\n",
            "[Epoch: 31]\n",
            "Train Loss: 0.0009\tTrain Accuracy: 0.9611\tVal Loss: 0.0039\tVal Accuracy: 0.8972\n",
            "\n",
            "Epoch: 32\n",
            "[Epoch: 32]\n",
            "Train Loss: 0.0008\tTrain Accuracy: 0.9629\tVal Loss: 0.0038\tVal Accuracy: 0.9021\n",
            "\n",
            "Epoch: 33\n",
            "[Epoch: 33]\n",
            "Train Loss: 0.0008\tTrain Accuracy: 0.9632\tVal Loss: 0.0046\tVal Accuracy: 0.8807\n",
            "\n",
            "Epoch: 34\n",
            "[Epoch: 34]\n",
            "Train Loss: 0.0008\tTrain Accuracy: 0.9649\tVal Loss: 0.0036\tVal Accuracy: 0.9062\n",
            "\n",
            "Epoch: 35\n",
            "[Epoch: 35]\n",
            "Train Loss: 0.0008\tTrain Accuracy: 0.9632\tVal Loss: 0.0036\tVal Accuracy: 0.9013\n",
            "\n",
            "Epoch: 36\n",
            "[Epoch: 36]\n",
            "Train Loss: 0.0008\tTrain Accuracy: 0.9658\tVal Loss: 0.0037\tVal Accuracy: 0.8960\n",
            "\n",
            "Epoch: 37\n",
            "[Epoch: 37]\n",
            "Train Loss: 0.0007\tTrain Accuracy: 0.9670\tVal Loss: 0.0036\tVal Accuracy: 0.9027\n",
            "\n",
            "Epoch: 38\n",
            "[Epoch: 38]\n",
            "Train Loss: 0.0008\tTrain Accuracy: 0.9631\tVal Loss: 0.0037\tVal Accuracy: 0.8931\n",
            "\n",
            "Epoch: 39\n",
            "[Epoch: 39]\n",
            "Train Loss: 0.0007\tTrain Accuracy: 0.9661\tVal Loss: 0.0038\tVal Accuracy: 0.9008\n",
            "\n",
            "Epoch: 40\n",
            "[Epoch: 40]\n",
            "Train Loss: 0.0007\tTrain Accuracy: 0.9691\tVal Loss: 0.0039\tVal Accuracy: 0.9002\n",
            "\n",
            "Epoch: 41\n",
            "[Epoch: 41]\n",
            "Train Loss: 0.0007\tTrain Accuracy: 0.9690\tVal Loss: 0.0034\tVal Accuracy: 0.9037\n",
            "\n",
            "Epoch: 42\n",
            "[Epoch: 42]\n",
            "Train Loss: 0.0007\tTrain Accuracy: 0.9691\tVal Loss: 0.0037\tVal Accuracy: 0.8966\n",
            "\n",
            "Epoch: 43\n",
            "[Epoch: 43]\n",
            "Train Loss: 0.0007\tTrain Accuracy: 0.9696\tVal Loss: 0.0042\tVal Accuracy: 0.8922\n",
            "\n",
            "Epoch: 44\n",
            "[Epoch: 44]\n",
            "Train Loss: 0.0007\tTrain Accuracy: 0.9694\tVal Loss: 0.0046\tVal Accuracy: 0.8820\n",
            "\n",
            "Epoch: 45\n",
            "[Epoch: 45]\n",
            "Train Loss: 0.0007\tTrain Accuracy: 0.9684\tVal Loss: 0.0040\tVal Accuracy: 0.8922\n",
            "\n",
            "Epoch: 46\n",
            "[Epoch: 46]\n",
            "Train Loss: 0.0007\tTrain Accuracy: 0.9710\tVal Loss: 0.0041\tVal Accuracy: 0.8951\n",
            "\n",
            "Epoch: 47\n",
            "[Epoch: 47]\n",
            "Train Loss: 0.0006\tTrain Accuracy: 0.9715\tVal Loss: 0.0041\tVal Accuracy: 0.8945\n",
            "\n",
            "Epoch: 48\n",
            "[Epoch: 48]\n",
            "Train Loss: 0.0006\tTrain Accuracy: 0.9714\tVal Loss: 0.0044\tVal Accuracy: 0.8926\n",
            "\n",
            "Epoch: 49\n",
            "[Epoch: 49]\n",
            "Train Loss: 0.0006\tTrain Accuracy: 0.9725\tVal Loss: 0.0036\tVal Accuracy: 0.9081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow9GkcWaUBBc"
      },
      "source": [
        "## fgsm train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiUjJiLEy5Ik"
      },
      "source": [
        "### eps=0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q9_rEOXy_jY",
        "outputId": "b47efae8-c433-4b82-8631-344b92b837ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for epoch in range(start_epoch, start_epoch+30):\n",
        "    train_loss, train_acc = train(resmodel, epoch)\n",
        "    adv_train_loss, adv_train_acc = fgsm_train(resmodel, epoch)\n",
        "    val_loss, val_acc = test(resmodel, epoch)\n",
        "    \n",
        "    print('[Epoch: {}]\\nTrain Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tAdv_Train Loss: {:.4f}\\tAdv_Train Accuracy: {:.4f}\\nVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
        "          format(epoch, train_loss, train_acc, adv_train_loss, adv_train_acc, val_loss, val_acc))\n",
        "    \n",
        "    if (epoch+1)%10 == 0:\n",
        "        torch.save({'model_state_dict': resmodel.state_dict(),},\n",
        "                    path + \"Adv_ResNet50_eps0.01_{}.pth\".format(str(epoch)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "\n",
            "Epoch: 0\n",
            "[Epoch: 0]\n",
            "Train Loss: 0.0113\tTrain Accuracy: 0.4799\tAdv_Train Loss: 0.0110\tAdv_Train Accuracy: 0.4954\n",
            "Val Loss: 0.0115\tVal Accuracy: 0.6115\n",
            "\n",
            "Epoch: 1\n",
            "\n",
            "Epoch: 1\n",
            "[Epoch: 1]\n",
            "Train Loss: 0.0073\tTrain Accuracy: 0.6678\tAdv_Train Loss: 0.0082\tAdv_Train Accuracy: 0.6219\n",
            "Val Loss: 0.0084\tVal Accuracy: 0.7129\n",
            "\n",
            "Epoch: 2\n",
            "\n",
            "Epoch: 2\n",
            "[Epoch: 2]\n",
            "Train Loss: 0.0050\tTrain Accuracy: 0.7759\tAdv_Train Loss: 0.0066\tAdv_Train Accuracy: 0.6996\n",
            "Val Loss: 0.0079\tVal Accuracy: 0.7487\n",
            "\n",
            "Epoch: 3\n",
            "\n",
            "Epoch: 3\n",
            "[Epoch: 3]\n",
            "Train Loss: 0.0039\tTrain Accuracy: 0.8276\tAdv_Train Loss: 0.0057\tAdv_Train Accuracy: 0.7394\n",
            "Val Loss: 0.0060\tVal Accuracy: 0.7936\n",
            "\n",
            "Epoch: 4\n",
            "\n",
            "Epoch: 4\n",
            "[Epoch: 4]\n",
            "Train Loss: 0.0032\tTrain Accuracy: 0.8604\tAdv_Train Loss: 0.0051\tAdv_Train Accuracy: 0.7696\n",
            "Val Loss: 0.0052\tVal Accuracy: 0.8264\n",
            "\n",
            "Epoch: 5\n",
            "\n",
            "Epoch: 5\n",
            "[Epoch: 5]\n",
            "Train Loss: 0.0027\tTrain Accuracy: 0.8821\tAdv_Train Loss: 0.0046\tAdv_Train Accuracy: 0.7865\n",
            "Val Loss: 0.0044\tVal Accuracy: 0.8503\n",
            "\n",
            "Epoch: 6\n",
            "\n",
            "Epoch: 6\n",
            "[Epoch: 6]\n",
            "Train Loss: 0.0023\tTrain Accuracy: 0.8990\tAdv_Train Loss: 0.0043\tAdv_Train Accuracy: 0.8033\n",
            "Val Loss: 0.0045\tVal Accuracy: 0.8484\n",
            "\n",
            "Epoch: 7\n",
            "\n",
            "Epoch: 7\n",
            "[Epoch: 7]\n",
            "Train Loss: 0.0020\tTrain Accuracy: 0.9139\tAdv_Train Loss: 0.0039\tAdv_Train Accuracy: 0.8206\n",
            "Val Loss: 0.0044\tVal Accuracy: 0.8534\n",
            "\n",
            "Epoch: 8\n",
            "\n",
            "Epoch: 8\n",
            "[Epoch: 8]\n",
            "Train Loss: 0.0017\tTrain Accuracy: 0.9241\tAdv_Train Loss: 0.0037\tAdv_Train Accuracy: 0.8298\n",
            "Val Loss: 0.0039\tVal Accuracy: 0.8680\n",
            "\n",
            "Epoch: 9\n",
            "\n",
            "Epoch: 9\n",
            "[Epoch: 9]\n",
            "Train Loss: 0.0015\tTrain Accuracy: 0.9329\tAdv_Train Loss: 0.0034\tAdv_Train Accuracy: 0.8412\n",
            "Val Loss: 0.0041\tVal Accuracy: 0.8589\n",
            "\n",
            "Epoch: 10\n",
            "\n",
            "Epoch: 10\n",
            "[Epoch: 10]\n",
            "Train Loss: 0.0014\tTrain Accuracy: 0.9398\tAdv_Train Loss: 0.0033\tAdv_Train Accuracy: 0.8469\n",
            "Val Loss: 0.0042\tVal Accuracy: 0.8609\n",
            "\n",
            "Epoch: 11\n",
            "\n",
            "Epoch: 11\n",
            "[Epoch: 11]\n",
            "Train Loss: 0.0012\tTrain Accuracy: 0.9469\tAdv_Train Loss: 0.0031\tAdv_Train Accuracy: 0.8557\n",
            "Val Loss: 0.0037\tVal Accuracy: 0.8752\n",
            "\n",
            "Epoch: 12\n",
            "\n",
            "Epoch: 12\n",
            "[Epoch: 12]\n",
            "Train Loss: 0.0011\tTrain Accuracy: 0.9510\tAdv_Train Loss: 0.0029\tAdv_Train Accuracy: 0.8642\n",
            "Val Loss: 0.0039\tVal Accuracy: 0.8751\n",
            "\n",
            "Epoch: 13\n",
            "\n",
            "Epoch: 13\n",
            "[Epoch: 13]\n",
            "Train Loss: 0.0010\tTrain Accuracy: 0.9558\tAdv_Train Loss: 0.0027\tAdv_Train Accuracy: 0.8723\n",
            "Val Loss: 0.0037\tVal Accuracy: 0.8833\n",
            "\n",
            "Epoch: 14\n",
            "\n",
            "Epoch: 14\n",
            "[Epoch: 14]\n",
            "Train Loss: 0.0009\tTrain Accuracy: 0.9603\tAdv_Train Loss: 0.0027\tAdv_Train Accuracy: 0.8736\n",
            "Val Loss: 0.0036\tVal Accuracy: 0.8810\n",
            "\n",
            "Epoch: 15\n",
            "\n",
            "Epoch: 15\n",
            "[Epoch: 15]\n",
            "Train Loss: 0.0008\tTrain Accuracy: 0.9626\tAdv_Train Loss: 0.0025\tAdv_Train Accuracy: 0.8816\n",
            "Val Loss: 0.0036\tVal Accuracy: 0.8843\n",
            "\n",
            "Epoch: 16\n",
            "\n",
            "Epoch: 16\n",
            "[Epoch: 16]\n",
            "Train Loss: 0.0008\tTrain Accuracy: 0.9656\tAdv_Train Loss: 0.0025\tAdv_Train Accuracy: 0.8838\n",
            "Val Loss: 0.0037\tVal Accuracy: 0.8814\n",
            "\n",
            "Epoch: 17\n",
            "\n",
            "Epoch: 17\n",
            "[Epoch: 17]\n",
            "Train Loss: 0.0007\tTrain Accuracy: 0.9702\tAdv_Train Loss: 0.0023\tAdv_Train Accuracy: 0.8925\n",
            "Val Loss: 0.0035\tVal Accuracy: 0.8875\n",
            "\n",
            "Epoch: 18\n",
            "\n",
            "Epoch: 18\n",
            "[Epoch: 18]\n",
            "Train Loss: 0.0006\tTrain Accuracy: 0.9727\tAdv_Train Loss: 0.0022\tAdv_Train Accuracy: 0.8955\n",
            "Val Loss: 0.0035\tVal Accuracy: 0.8909\n",
            "\n",
            "Epoch: 19\n",
            "\n",
            "Epoch: 19\n",
            "[Epoch: 19]\n",
            "Train Loss: 0.0006\tTrain Accuracy: 0.9739\tAdv_Train Loss: 0.0022\tAdv_Train Accuracy: 0.8979\n",
            "Val Loss: 0.0037\tVal Accuracy: 0.8857\n",
            "\n",
            "Epoch: 20\n",
            "\n",
            "Epoch: 20\n",
            "[Epoch: 20]\n",
            "Train Loss: 0.0006\tTrain Accuracy: 0.9753\tAdv_Train Loss: 0.0021\tAdv_Train Accuracy: 0.9035\n",
            "Val Loss: 0.0036\tVal Accuracy: 0.8857\n",
            "\n",
            "Epoch: 21\n",
            "\n",
            "Epoch: 21\n",
            "[Epoch: 21]\n",
            "Train Loss: 0.0005\tTrain Accuracy: 0.9771\tAdv_Train Loss: 0.0020\tAdv_Train Accuracy: 0.9049\n",
            "Val Loss: 0.0035\tVal Accuracy: 0.8964\n",
            "\n",
            "Epoch: 22\n",
            "\n",
            "Epoch: 22\n",
            "[Epoch: 22]\n",
            "Train Loss: 0.0004\tTrain Accuracy: 0.9814\tAdv_Train Loss: 0.0020\tAdv_Train Accuracy: 0.9074\n",
            "Val Loss: 0.0032\tVal Accuracy: 0.9000\n",
            "\n",
            "Epoch: 23\n",
            "\n",
            "Epoch: 23\n",
            "[Epoch: 23]\n",
            "Train Loss: 0.0004\tTrain Accuracy: 0.9810\tAdv_Train Loss: 0.0020\tAdv_Train Accuracy: 0.9081\n",
            "Val Loss: 0.0035\tVal Accuracy: 0.8936\n",
            "\n",
            "Epoch: 24\n",
            "\n",
            "Epoch: 24\n",
            "[Epoch: 24]\n",
            "Train Loss: 0.0004\tTrain Accuracy: 0.9819\tAdv_Train Loss: 0.0019\tAdv_Train Accuracy: 0.9130\n",
            "Val Loss: 0.0032\tVal Accuracy: 0.9021\n",
            "\n",
            "Epoch: 25\n",
            "\n",
            "Epoch: 25\n",
            "[Epoch: 25]\n",
            "Train Loss: 0.0004\tTrain Accuracy: 0.9817\tAdv_Train Loss: 0.0018\tAdv_Train Accuracy: 0.9137\n",
            "Val Loss: 0.0037\tVal Accuracy: 0.8863\n",
            "\n",
            "Epoch: 26\n",
            "\n",
            "Epoch: 26\n",
            "[Epoch: 26]\n",
            "Train Loss: 0.0004\tTrain Accuracy: 0.9835\tAdv_Train Loss: 0.0018\tAdv_Train Accuracy: 0.9138\n",
            "Val Loss: 0.0032\tVal Accuracy: 0.8984\n",
            "\n",
            "Epoch: 27\n",
            "\n",
            "Epoch: 27\n",
            "[Epoch: 27]\n",
            "Train Loss: 0.0004\tTrain Accuracy: 0.9839\tAdv_Train Loss: 0.0018\tAdv_Train Accuracy: 0.9156\n",
            "Val Loss: 0.0035\tVal Accuracy: 0.8915\n",
            "\n",
            "Epoch: 28\n",
            "\n",
            "Epoch: 28\n",
            "[Epoch: 28]\n",
            "Train Loss: 0.0003\tTrain Accuracy: 0.9862\tAdv_Train Loss: 0.0018\tAdv_Train Accuracy: 0.9169\n",
            "Val Loss: 0.0032\tVal Accuracy: 0.9058\n",
            "\n",
            "Epoch: 29\n",
            "\n",
            "Epoch: 29\n",
            "[Epoch: 29]\n",
            "Train Loss: 0.0003\tTrain Accuracy: 0.9866\tAdv_Train Loss: 0.0017\tAdv_Train Accuracy: 0.9188\n",
            "Val Loss: 0.0034\tVal Accuracy: 0.8962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svHabjchy0fP"
      },
      "source": [
        "### eps=0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLAgiRPJpsMO",
        "outputId": "2d506293-6e2e-4cc1-8bf7-ec6dc9a1d7d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(start_epoch, start_epoch+50):\n",
        "    train_loss, train_acc = train(resmodel, epoch)\n",
        "    adv_train_loss, adv_train_acc = fgsm_train(resmodel, epoch)\n",
        "    val_loss, val_acc = test(resmodel, epoch)\n",
        "    \n",
        "    print('[Epoch: {}]\\nTrain Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tAdv_Train Loss: {:.4f}\\tAdv_Train Accuracy: {:.4f}\\nVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
        "          format(epoch, train_loss, train_acc, adv_train_loss, adv_train_acc, val_loss, val_acc))\n",
        "    \n",
        "    if (epoch+1)%10 == 0:\n",
        "        torch.save({'model_state_dict': resmodel.state_dict(),},\n",
        "                    path + \"Adv_ResNet50_{}.pth\".format(str(epoch)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "\n",
            "Epoch: 0\n",
            "[Epoch: 0]\n",
            "Train Loss: 0.0075\tTrain Accuracy: 0.6590\tAdv_Train Loss: 0.0141\tAdv_Train Accuracy: 0.3317\n",
            "Val Loss: 0.0096\tVal Accuracy: 0.6717\n",
            "\n",
            "Epoch: 1\n",
            "\n",
            "Epoch: 1\n",
            "[Epoch: 1]\n",
            "Train Loss: 0.0057\tTrain Accuracy: 0.7436\tAdv_Train Loss: 0.0128\tAdv_Train Accuracy: 0.3875\n",
            "Val Loss: 0.0085\tVal Accuracy: 0.7025\n",
            "\n",
            "Epoch: 2\n",
            "\n",
            "Epoch: 2\n",
            "[Epoch: 2]\n",
            "Train Loss: 0.0046\tTrain Accuracy: 0.7939\tAdv_Train Loss: 0.0119\tAdv_Train Accuracy: 0.4296\n",
            "Val Loss: 0.0077\tVal Accuracy: 0.7319\n",
            "\n",
            "Epoch: 3\n",
            "\n",
            "Epoch: 3\n",
            "[Epoch: 3]\n",
            "Train Loss: 0.0040\tTrain Accuracy: 0.8239\tAdv_Train Loss: 0.0106\tAdv_Train Accuracy: 0.4880\n",
            "Val Loss: 0.0072\tVal Accuracy: 0.7495\n",
            "\n",
            "Epoch: 4\n",
            "\n",
            "Epoch: 4\n",
            "[Epoch: 4]\n",
            "Train Loss: 0.0033\tTrain Accuracy: 0.8529\tAdv_Train Loss: 0.0094\tAdv_Train Accuracy: 0.5574\n",
            "Val Loss: 0.0073\tVal Accuracy: 0.7622\n",
            "\n",
            "Epoch: 5\n",
            "\n",
            "Epoch: 5\n",
            "[Epoch: 5]\n",
            "Train Loss: 0.0029\tTrain Accuracy: 0.8733\tAdv_Train Loss: 0.0088\tAdv_Train Accuracy: 0.5882\n",
            "Val Loss: 0.0068\tVal Accuracy: 0.7696\n",
            "\n",
            "Epoch: 6\n",
            "\n",
            "Epoch: 6\n",
            "[Epoch: 6]\n",
            "Train Loss: 0.0025\tTrain Accuracy: 0.8870\tAdv_Train Loss: 0.0083\tAdv_Train Accuracy: 0.6119\n",
            "Val Loss: 0.0065\tVal Accuracy: 0.7879\n",
            "\n",
            "Epoch: 7\n",
            "\n",
            "Epoch: 7\n",
            "[Epoch: 7]\n",
            "Train Loss: 0.0022\tTrain Accuracy: 0.8992\tAdv_Train Loss: 0.0079\tAdv_Train Accuracy: 0.6365\n",
            "Val Loss: 0.0062\tVal Accuracy: 0.7927\n",
            "\n",
            "Epoch: 8\n",
            "\n",
            "Epoch: 8\n",
            "[Epoch: 8]\n",
            "Train Loss: 0.0020\tTrain Accuracy: 0.9096\tAdv_Train Loss: 0.0075\tAdv_Train Accuracy: 0.6584\n",
            "Val Loss: 0.0062\tVal Accuracy: 0.7841\n",
            "\n",
            "Epoch: 9\n",
            "\n",
            "Epoch: 9\n",
            "[Epoch: 9]\n",
            "Train Loss: 0.0018\tTrain Accuracy: 0.9201\tAdv_Train Loss: 0.0091\tAdv_Train Accuracy: 0.5685\n",
            "Val Loss: 0.0057\tVal Accuracy: 0.8086\n",
            "\n",
            "Epoch: 10\n",
            "\n",
            "Epoch: 10\n",
            "[Epoch: 10]\n",
            "Train Loss: 0.0018\tTrain Accuracy: 0.9199\tAdv_Train Loss: 0.0075\tAdv_Train Accuracy: 0.6584\n",
            "Val Loss: 0.0057\tVal Accuracy: 0.8056\n",
            "\n",
            "Epoch: 11\n",
            "\n",
            "Epoch: 11\n",
            "[Epoch: 11]\n",
            "Train Loss: 0.0016\tTrain Accuracy: 0.9285\tAdv_Train Loss: 0.0071\tAdv_Train Accuracy: 0.6793\n",
            "Val Loss: 0.0066\tVal Accuracy: 0.7746\n",
            "\n",
            "Epoch: 12\n",
            "\n",
            "Epoch: 12\n",
            "[Epoch: 12]\n",
            "Train Loss: 0.0014\tTrain Accuracy: 0.9370\tAdv_Train Loss: 0.0071\tAdv_Train Accuracy: 0.6792\n",
            "Val Loss: 0.0052\tVal Accuracy: 0.8196\n",
            "\n",
            "Epoch: 13\n",
            "\n",
            "Epoch: 13\n",
            "[Epoch: 13]\n",
            "Train Loss: 0.0012\tTrain Accuracy: 0.9460\tAdv_Train Loss: 0.0064\tAdv_Train Accuracy: 0.7135\n",
            "Val Loss: 0.0050\tVal Accuracy: 0.8193\n",
            "\n",
            "Epoch: 14\n",
            "\n",
            "Epoch: 14\n",
            "[Epoch: 14]\n",
            "Train Loss: 0.0011\tTrain Accuracy: 0.9482\tAdv_Train Loss: 0.0063\tAdv_Train Accuracy: 0.7196\n",
            "Val Loss: 0.0050\tVal Accuracy: 0.8282\n",
            "\n",
            "Epoch: 15\n",
            "\n",
            "Epoch: 15\n",
            "[Epoch: 15]\n",
            "Train Loss: 0.0010\tTrain Accuracy: 0.9529\tAdv_Train Loss: 0.0061\tAdv_Train Accuracy: 0.7325\n",
            "Val Loss: 0.0047\tVal Accuracy: 0.8443\n",
            "\n",
            "Epoch: 16\n",
            "\n",
            "Epoch: 16\n",
            "[Epoch: 16]\n",
            "Train Loss: 0.0010\tTrain Accuracy: 0.9545\tAdv_Train Loss: 0.0059\tAdv_Train Accuracy: 0.7388\n",
            "Val Loss: 0.0048\tVal Accuracy: 0.8373\n",
            "\n",
            "Epoch: 17\n",
            "\n",
            "Epoch: 17\n",
            "[Epoch: 17]\n",
            "Train Loss: 0.0009\tTrain Accuracy: 0.9599\tAdv_Train Loss: 0.0057\tAdv_Train Accuracy: 0.7504\n",
            "Val Loss: 0.0046\tVal Accuracy: 0.8439\n",
            "\n",
            "Epoch: 18\n",
            "\n",
            "Epoch: 18\n",
            "[Epoch: 18]\n",
            "Train Loss: 0.0008\tTrain Accuracy: 0.9620\tAdv_Train Loss: 0.0057\tAdv_Train Accuracy: 0.7531\n",
            "Val Loss: 0.0051\tVal Accuracy: 0.8085\n",
            "\n",
            "Epoch: 19\n",
            "\n",
            "Epoch: 19\n",
            "[Epoch: 19]\n",
            "Train Loss: 0.0008\tTrain Accuracy: 0.9627\tAdv_Train Loss: 0.0055\tAdv_Train Accuracy: 0.7567\n",
            "Val Loss: 0.0044\tVal Accuracy: 0.8512\n",
            "\n",
            "Epoch: 20\n",
            "\n",
            "Epoch: 20\n",
            "[Epoch: 20]\n",
            "Train Loss: 0.0008\tTrain Accuracy: 0.9656\tAdv_Train Loss: 0.0053\tAdv_Train Accuracy: 0.7698\n",
            "Val Loss: 0.0046\tVal Accuracy: 0.8399\n",
            "\n",
            "Epoch: 21\n",
            "\n",
            "Epoch: 21\n",
            "[Epoch: 21]\n",
            "Train Loss: 0.0007\tTrain Accuracy: 0.9675\tAdv_Train Loss: 0.0052\tAdv_Train Accuracy: 0.7735\n",
            "Val Loss: 0.0043\tVal Accuracy: 0.8546\n",
            "\n",
            "Epoch: 22\n",
            "\n",
            "Epoch: 22\n",
            "[Epoch: 22]\n",
            "Train Loss: 0.0007\tTrain Accuracy: 0.9685\tAdv_Train Loss: 0.0050\tAdv_Train Accuracy: 0.7806\n",
            "Val Loss: 0.0049\tVal Accuracy: 0.8269\n",
            "\n",
            "Epoch: 23\n",
            "\n",
            "Epoch: 23\n",
            "[Epoch: 23]\n",
            "Train Loss: 0.0006\tTrain Accuracy: 0.9714\tAdv_Train Loss: 0.0054\tAdv_Train Accuracy: 0.7645\n",
            "Val Loss: 0.0053\tVal Accuracy: 0.8378\n",
            "\n",
            "Epoch: 24\n",
            "\n",
            "Epoch: 24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-6e45e50b8177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0madv_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv_train_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madv_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-65148f13b485>\u001b[0m in \u001b[0;36madv_train\u001b[0;34m(net, epoch, eps)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mnew_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnew_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfFiuFt_UH_y"
      },
      "source": [
        "## pgd train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7xwMCN1UKeG",
        "outputId": "0822c879-e233-4c3d-82d9-21e8b6027a48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(start_epoch, start_epoch+30):\n",
        "    adv_train_loss, adv_train_acc = pgd_train(resmodel, epoch)\n",
        "    val_loss, val_acc = test(resmodel, epoch)\n",
        "    \n",
        "    print('[Epoch: {}]\\nAdv_Train Loss: {:.4f}\\tAdv_Train Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
        "          format(epoch, adv_train_loss, adv_train_acc, val_loss, val_acc))\n",
        "    \n",
        "    if (epoch+1)%10 == 0:\n",
        "        torch.save({'model_state_dict': resmodel.state_dict(),},\n",
        "                    path + \"PGD_ResNet50_{}.pth\".format(str(epoch)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "278.2911500930786\n",
            "[Epoch: 0]\n",
            "Adv_Train Loss: 1.3248\tAdv_Train Accuracy: 0.4844\tVal Loss: 0.0062\tVal Accuracy: 0.8229\n",
            "277.60825300216675\n",
            "[Epoch: 1]\n",
            "Adv_Train Loss: 1.2920\tAdv_Train Accuracy: 0.5032\tVal Loss: 0.0062\tVal Accuracy: 0.8067\n",
            "277.5205228328705\n",
            "[Epoch: 2]\n",
            "Adv_Train Loss: 1.2489\tAdv_Train Accuracy: 0.5185\tVal Loss: 0.0071\tVal Accuracy: 0.7766\n",
            "277.53821444511414\n",
            "[Epoch: 3]\n",
            "Adv_Train Loss: 1.2150\tAdv_Train Accuracy: 0.5302\tVal Loss: 0.0067\tVal Accuracy: 0.7932\n",
            "277.5419387817383\n",
            "[Epoch: 4]\n",
            "Adv_Train Loss: 1.1983\tAdv_Train Accuracy: 0.5368\tVal Loss: 0.0065\tVal Accuracy: 0.8036\n",
            "277.5575520992279\n",
            "[Epoch: 5]\n",
            "Adv_Train Loss: 1.1725\tAdv_Train Accuracy: 0.5442\tVal Loss: 0.0063\tVal Accuracy: 0.8073\n",
            "277.56264328956604\n",
            "[Epoch: 6]\n",
            "Adv_Train Loss: 1.1576\tAdv_Train Accuracy: 0.5548\tVal Loss: 0.0067\tVal Accuracy: 0.8017\n",
            "277.5425045490265\n",
            "[Epoch: 7]\n",
            "Adv_Train Loss: 1.1449\tAdv_Train Accuracy: 0.5585\tVal Loss: 0.0071\tVal Accuracy: 0.7785\n",
            "277.55950021743774\n",
            "[Epoch: 8]\n",
            "Adv_Train Loss: 1.1357\tAdv_Train Accuracy: 0.5615\tVal Loss: 0.0063\tVal Accuracy: 0.8022\n",
            "277.5678050518036\n",
            "[Epoch: 9]\n",
            "Adv_Train Loss: 1.1238\tAdv_Train Accuracy: 0.5653\tVal Loss: 0.0062\tVal Accuracy: 0.7999\n",
            "277.49735164642334\n",
            "[Epoch: 10]\n",
            "Adv_Train Loss: 1.1108\tAdv_Train Accuracy: 0.5714\tVal Loss: 0.0060\tVal Accuracy: 0.8095\n",
            "277.5796072483063\n",
            "[Epoch: 11]\n",
            "Adv_Train Loss: 1.1009\tAdv_Train Accuracy: 0.5733\tVal Loss: 0.0058\tVal Accuracy: 0.8137\n",
            "277.577082157135\n",
            "[Epoch: 12]\n",
            "Adv_Train Loss: 1.0944\tAdv_Train Accuracy: 0.5779\tVal Loss: 0.0063\tVal Accuracy: 0.8095\n",
            "277.56158232688904\n",
            "[Epoch: 13]\n",
            "Adv_Train Loss: 1.0820\tAdv_Train Accuracy: 0.5832\tVal Loss: 0.0064\tVal Accuracy: 0.7856\n",
            "277.523161649704\n",
            "[Epoch: 14]\n",
            "Adv_Train Loss: 1.0735\tAdv_Train Accuracy: 0.5831\tVal Loss: 0.0063\tVal Accuracy: 0.8130\n",
            "277.67168831825256\n",
            "[Epoch: 15]\n",
            "Adv_Train Loss: 1.0700\tAdv_Train Accuracy: 0.5867\tVal Loss: 0.0059\tVal Accuracy: 0.8142\n",
            "277.3742377758026\n",
            "[Epoch: 16]\n",
            "Adv_Train Loss: 1.0555\tAdv_Train Accuracy: 0.5905\tVal Loss: 0.0058\tVal Accuracy: 0.8136\n",
            "277.5009980201721\n",
            "[Epoch: 17]\n",
            "Adv_Train Loss: 1.0475\tAdv_Train Accuracy: 0.5954\tVal Loss: 0.0058\tVal Accuracy: 0.8168\n",
            "277.4159173965454\n",
            "[Epoch: 18]\n",
            "Adv_Train Loss: 1.0464\tAdv_Train Accuracy: 0.5927\tVal Loss: 0.0060\tVal Accuracy: 0.8157\n",
            "277.4381444454193\n",
            "[Epoch: 19]\n",
            "Adv_Train Loss: 1.0295\tAdv_Train Accuracy: 0.6002\tVal Loss: 0.0058\tVal Accuracy: 0.8161\n",
            "277.3919143676758\n",
            "[Epoch: 20]\n",
            "Adv_Train Loss: 1.0231\tAdv_Train Accuracy: 0.6028\tVal Loss: 0.0059\tVal Accuracy: 0.8096\n",
            "277.4507215023041\n",
            "[Epoch: 21]\n",
            "Adv_Train Loss: 1.0107\tAdv_Train Accuracy: 0.6084\tVal Loss: 0.0057\tVal Accuracy: 0.8178\n",
            "277.4062089920044\n",
            "[Epoch: 22]\n",
            "Adv_Train Loss: 0.9988\tAdv_Train Accuracy: 0.6105\tVal Loss: 0.0056\tVal Accuracy: 0.8317\n",
            "277.4715247154236\n",
            "[Epoch: 23]\n",
            "Adv_Train Loss: 0.9940\tAdv_Train Accuracy: 0.6142\tVal Loss: 0.0056\tVal Accuracy: 0.8132\n",
            "277.5982987880707\n",
            "[Epoch: 24]\n",
            "Adv_Train Loss: 0.9789\tAdv_Train Accuracy: 0.6177\tVal Loss: 0.0054\tVal Accuracy: 0.8236\n",
            "277.84157276153564\n",
            "[Epoch: 25]\n",
            "Adv_Train Loss: 0.9715\tAdv_Train Accuracy: 0.6212\tVal Loss: 0.0057\tVal Accuracy: 0.8150\n",
            "277.44821858406067\n",
            "[Epoch: 26]\n",
            "Adv_Train Loss: 0.9657\tAdv_Train Accuracy: 0.6242\tVal Loss: 0.0055\tVal Accuracy: 0.8266\n",
            "277.4286150932312\n",
            "[Epoch: 27]\n",
            "Adv_Train Loss: 0.9350\tAdv_Train Accuracy: 0.6356\tVal Loss: 0.0053\tVal Accuracy: 0.8305\n",
            "277.61517119407654\n",
            "[Epoch: 28]\n",
            "Adv_Train Loss: 0.6703\tAdv_Train Accuracy: 0.7456\tVal Loss: 0.0051\tVal Accuracy: 0.8356\n",
            "277.6081614494324\n",
            "[Epoch: 29]\n",
            "Adv_Train Loss: 0.2745\tAdv_Train Accuracy: 0.9046\tVal Loss: 0.0064\tVal Accuracy: 0.8175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wic9pXEboiqx"
      },
      "source": [
        "## FGSM attack\n",
        "------------------\n",
        "batch size = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf1A73tsouKj",
        "outputId": "11f58157-be82-4498-d136-7b3a70f9b2c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "epsilons = [0, .05, .1, .15, .2, .25, .3]\n",
        "# use_cuda=True\n",
        "\n",
        "\"\"\"Load Model\"\"\"\n",
        "# epoch=7, the highest AUC score\n",
        "modelpath = path + \"Adc_ResNet50_19.pth\"\n",
        "loadmodel = ResNet50().to(device)\n",
        "loadmodel.load_state_dict(torch.load(modelpath)['model_state_dict'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO92DzJY1IVT"
      },
      "source": [
        "attackloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=1, shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0oTBHOKom50"
      },
      "source": [
        "# FGSM attack code\n",
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "    # Collect the element-wise sign of the data gradient\n",
        "    sign_data_grad = data_grad.sign()\n",
        "    # Create the perturbed image by adjusting each pixel of the input image\n",
        "    perturbed_image = image + epsilon*sign_data_grad\n",
        "    # Adding clipping to maintain [0,1] range\n",
        "    # perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
        "    # Return the perturbed image\n",
        "    return perturbed_image"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnMLkSBe-aBu",
        "outputId": "aefd2fb3-181f-42d9-ea27-d71351c03369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "\"\"\"free train\"\"\"\n",
        "\n",
        "accuracies = []\n",
        "# examples = []\n",
        "\n",
        "# Run test for each epsilon\n",
        "for eps in epsilons:\n",
        "    accuracy = testattack(loadmodel, attackloader, eps)\n",
        "    accuracies.append(accuracy)\n",
        "    print(accuracies)\n",
        "    # examples.append(example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epsilon: 0\tTest Accuracy = 9081 / 10000 = 0.9081\n",
            "[0.9081]\n",
            "Epsilon: 0.05\tTest Accuracy = 2477 / 10000 = 0.2477\n",
            "[0.9081, 0.2477]\n",
            "Epsilon: 0.1\tTest Accuracy = 1323 / 10000 = 0.1323\n",
            "[0.9081, 0.2477, 0.1323]\n",
            "Epsilon: 0.15\tTest Accuracy = 1035 / 10000 = 0.1035\n",
            "[0.9081, 0.2477, 0.1323, 0.1035]\n",
            "Epsilon: 0.2\tTest Accuracy = 933 / 10000 = 0.0933\n",
            "[0.9081, 0.2477, 0.1323, 0.1035, 0.0933]\n",
            "Epsilon: 0.25\tTest Accuracy = 888 / 10000 = 0.0888\n",
            "[0.9081, 0.2477, 0.1323, 0.1035, 0.0933, 0.0888]\n",
            "Epsilon: 0.3\tTest Accuracy = 854 / 10000 = 0.0854\n",
            "[0.9081, 0.2477, 0.1323, 0.1035, 0.0933, 0.0888, 0.0854]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZiwbYANotQl",
        "outputId": "91b6f151-2fa3-449f-96dc-5a8f5bafb382",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "accuracies = []\n",
        "examples = []\n",
        "\n",
        "# Run test for each epsilon\n",
        "for eps in epsilons:\n",
        "    acc, ex = attack(loadmodel, attackloader, eps)\n",
        "    accuracies.append(acc)\n",
        "    examples.append(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epsilon: 0\tTest Accuracy = 4932 / 10000 = 0.4932\n",
            "Epsilon: 0.05\tTest Accuracy = 3525 / 10000 = 0.3525\n",
            "Epsilon: 0.1\tTest Accuracy = 2746 / 10000 = 0.2746\n",
            "Epsilon: 0.15\tTest Accuracy = 2338 / 10000 = 0.2338\n",
            "Epsilon: 0.2\tTest Accuracy = 2089 / 10000 = 0.2089\n",
            "Epsilon: 0.25\tTest Accuracy = 1925 / 10000 = 0.1925\n",
            "Epsilon: 0.3\tTest Accuracy = 1820 / 10000 = 0.182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvCxM0T_Y3LQ",
        "outputId": "db170df7-c6f9-4c8a-84ab-37111847babe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "\"\"\"fgsm train\"\"\"\n",
        "accuracies = []\n",
        "\n",
        "for eps in epsilons:\n",
        "    acc = testattack(loadmodel, attackloader, eps)\n",
        "    accuracies.append(acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epsilon: 0\tTest Accuracy = 8404 / 10000 = 0.8404\n",
            "Epsilon: 0.05\tTest Accuracy = 6389 / 10000 = 0.6389\n",
            "Epsilon: 0.1\tTest Accuracy = 4542 / 10000 = 0.4542\n",
            "Epsilon: 0.15\tTest Accuracy = 3190 / 10000 = 0.319\n",
            "Epsilon: 0.2\tTest Accuracy = 2227 / 10000 = 0.2227\n",
            "Epsilon: 0.25\tTest Accuracy = 1606 / 10000 = 0.1606\n",
            "Epsilon: 0.3\tTest Accuracy = 1221 / 10000 = 0.1221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgSIgstBUPjn"
      },
      "source": [
        "## PGD attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmIVhM43UO8x"
      },
      "source": [
        "def pgd_attack(model, images, labels, eps=0.3, alpha=2/255, iters=5) :\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "        \n",
        "    ori_images = images.data\n",
        "        \n",
        "    for i in range(iters) :    \n",
        "        images.requires_grad = True\n",
        "        outputs = model(images)\n",
        "\n",
        "        model.zero_grad()\n",
        "        cost = loss(outputs, labels).to(device)\n",
        "        cost.backward()\n",
        "\n",
        "        adv_images = images + alpha*images.grad.sign()\n",
        "        eta = torch.clamp(adv_images - ori_images, min=-eps, max=eps)\n",
        "        images = torch.clamp(ori_images + eta, min=0, max=1).detach_()\n",
        "            \n",
        "    return images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBYaPa3cWUJL",
        "outputId": "f4a9265d-460c-45a6-91d9-add0b0d40431",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\"\"\"Load Model\"\"\"\n",
        "# epoch=29, Adv_Train Accuracy: 0.9046\tVal Accuracy: 0.8175\n",
        "modelpath = path + \"PGD_ResNet50_29.pth\"\n",
        "loadmodel = ResNet50().to(device)\n",
        "loadmodel.load_state_dict(torch.load(modelpath)['model_state_dict'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP8oTfBrF2dc",
        "outputId": "eb0254f2-aa45-4fdf-89cd-89c92e7e4de4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_acc = testattack2(loadmodel, attackloader)\n",
        "test_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attack Image & Predicted Label\n",
            "Accuracy of test text: 5.870000 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0587"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9awOLBXqQQPT"
      },
      "source": [
        "# Draft"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw1Dcptq4QMe"
      },
      "source": [
        "\"\"\"ResNet50 Model\"\"\"\n",
        "resmodel = ResNet50(Bottleneck, [3,4,6,3]).to(device)\n",
        "resmodel.apply(init_weights)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.SGD(resmodel.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "optimizer = torch.optim.Adam(resmodel.parameters(), lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "classification train and dev\n",
        "\"\"\"\n",
        "def train(model, data_loader, class_test_loader, verify_test_loader, scheduler):\n",
        "    model.train()\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(numEpochs):\n",
        "        for batch_num, (feats, labels) in enumerate(data_loader):\n",
        "            feats, labels = feats.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(feats)[1]\n",
        "                loss = criterion(outputs, labels.long())\n",
        "            \n",
        "            # loss.backward()\n",
        "            scaler.scale(loss).backward()\n",
        "            \n",
        "            # optimizer.step()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            del feats\n",
        "            del labels\n",
        "            del loss\n",
        "\n",
        "        print('Classification')\n",
        "        train_loss, train_acc = test_classify(model, data_loader)\n",
        "        val_loss, val_acc = test_classify(model, class_test_loader)\n",
        "        print('[Epoch: {}]\\nTrain Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
        "              format(epoch, train_loss, train_acc, val_loss, val_acc))\n",
        "            # task = 'Verification'\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        torch.save({'model_state_dict': model.state_dict(),},\n",
        "                    path + \"ResNet50_{}.pth\".format(str(epoch)))\n",
        "            \n",
        "\n",
        "        # else: task = 'Classification'\n",
        "        if (epoch+1)%2 == 0:\n",
        "            print('Verification')\n",
        "            auc = test_verify(model, verify_test_loader)\n",
        "            print('[Epoch: {}]\\tAUC: {:.4f}'.format(epoch, auc))\n",
        "\n",
        "def test_classify(model, test_loader):\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = []\n",
        "    accuracy = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_num, (feats, labels) in enumerate(test_loader):\n",
        "            feats, labels = feats.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(feats)[1]\n",
        "            loss = criterion(outputs, labels.long()).detach()\n",
        "            test_loss.extend([loss.item()]*feats.size()[0])\n",
        "\n",
        "            # transform the prediction to one-hot form\n",
        "            _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
        "            pred_labels = pred_labels.view(-1)\n",
        "            \n",
        "            accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "            total += len(labels)\n",
        "            del feats\n",
        "            del labels\n",
        "\n",
        "    acc = accuracy/total\n",
        "    avg_loss = np.mean(test_loss)\n",
        "    \n",
        "    model.train()\n",
        "    return avg_loss, acc\n",
        "\n",
        "\n",
        "def test_verify(model, test_loader):\n",
        "    model.eval()\n",
        "\n",
        "    score_list = []\n",
        "    truth_list = []\n",
        "\n",
        "    for img1, img2, label1, label2, truth in test_loader:\n",
        "        \n",
        "        img1 = img1.to(device)\n",
        "        img2 = img2.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            feat1 = model(img1)[0].cpu().numpy()\n",
        "            feat2 = model(img2)[0].cpu().numpy()\n",
        "\n",
        "            value = cosine_similarity(feat1,feat2).diagonal()\n",
        "            score_list.append(value)\n",
        "\n",
        "            truth_list.append(truth.item())\n",
        "\n",
        "            del img1\n",
        "            del img2\n",
        "            del label1\n",
        "            del label2\n",
        "            del truth\n",
        "\n",
        "    similarity = np.array(score_list)\n",
        "    true_label = np.array(truth_list)\n",
        "    auc = roc_auc_score(true_label, similarity)\n",
        "\n",
        "    model.train()\n",
        "    return auc\n",
        "\n",
        "\n",
        "resmodel.train()\n",
        "train(resmodel, train_dataloader, dev_dataloader, verify_val_loader, scheduler)\n",
        "\n",
        "\n",
        "\"\"\"Load Model\"\"\"\n",
        "# epoch=7, the highest AUC score\n",
        "modelpath = path + \"ResNet50_7.pth\"\n",
        "loadmodel = ResNet50(Bottleneck, [3,4,6,3]).to(device)\n",
        "loadmodel.load_state_dict(torch.load(modelpath)['model_state_dict'])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Compute verify_val AUC and Write verify_test Prediction Results\n",
        "\"\"\"\n",
        "def verification_val(model, data_loader, device):\n",
        "    model.eval()\n",
        "\n",
        "    score_list = []\n",
        "    truth_list = []\n",
        "\n",
        "    for img1, img2, label1, label2, truth in data_loader:\n",
        "        \n",
        "        img1 = img1.to(device)\n",
        "        img2 = img2.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            feat1 = model(img1).cpu().numpy()\n",
        "            feat2 = model(img2).cpu().numpy()\n",
        "\n",
        "            value = cosine_similarity(feat1,feat2).diagonal()\n",
        "            score_list.append(value)\n",
        "\n",
        "            truth_list.append(truth.item())\n",
        "\n",
        "    similarity = np.array(score_list)\n",
        "    true_label = np.array(truth_list)\n",
        "    auc = roc_auc_score(true_label, similarity)\n",
        "    return auc\n",
        "\n",
        "def verification_test(model, data_loader, device):\n",
        "    model.eval()\n",
        "\n",
        "    name_list1 = []\n",
        "    name_list2 = []\n",
        "    score_list = []\n",
        "\n",
        "    for fig1, fig2, name1, name2 in data_loader:\n",
        "        \n",
        "        fig1 = fig1.to(device)\n",
        "        fig2 = fig2.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            feat1 = model(fig1).cpu().numpy()\n",
        "            feat2 = model(fig2).cpu().numpy()\n",
        "\n",
        "        value = cosine_similarity(feat1, feat2).diagonal()\n",
        "        \n",
        "        name_list1.append(name1)\n",
        "        name_list2.append(name2)\n",
        "        score_list.append(value)\n",
        "    return name_list1, name_list2, score_list\n",
        "\n",
        "\n",
        "verify_val_auc = verification_val(loadmodel, verify_val_loader, device)\n",
        "\n",
        "names, scores = verification_test(loadmodel, verify_test_loader, device)\n",
        "verify_results = pd.DataFrame({'Id':names, 'Category': scores})\n",
        "verify_results.to_csv('results.csv', index=False)\n",
        "\n",
        "# !pip install kaggle \n",
        "# !kaggle competitions submit -c 11-785-fall-20-homework-2-part-2 -f verify_results.csv -m \"Message\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}