{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of SAP-like networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8_VO9K36G2Q"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrR1jH7N6Gl0"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import argparse\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "path = \"/content/gdrive/My Drive/GameTheory/\"\n",
        "\n",
        "gpu = True\n",
        "gpu = gpu and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if gpu else \"cpu\")\n",
        "\n",
        "'''hparameters'''\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "learningRate = 0.01\n",
        "weightDecay = 5e-4\n",
        "\n",
        "best_acc = 0     # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN9CTmoq53uy"
      },
      "source": [
        "# CIFAR10 Data\r\n",
        "------------------------------------\r\n",
        "transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIgCxd8kvFlI"
      },
      "source": [
        "print('==> Preparing data..')\n",
        "transformtrain = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transformtest = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "\"\"\"[-1,1]\"\"\"\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transformtrain)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=256, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transformtest)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "\n",
        "attackloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Uo3qIIShtNk"
      },
      "source": [
        "# Attack Images Generation\r\n",
        "------------------------\r\n",
        "batch_size=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAcIgon0ipHi"
      },
      "source": [
        "## FGSM\r\n",
        "-------------------\r\n",
        "epsilon=2/256"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHBAbBEFh0Ks"
      },
      "source": [
        "# FGSM attack code\r\n",
        "\"\"\"epsilon=2/256\"\"\"\r\n",
        "\r\n",
        "def fgsm_attack(image, epsilon, data_grad):\r\n",
        "    # Collect the element-wise sign of the data gradient\r\n",
        "    sign_data_grad = data_grad.sign()\r\n",
        "    # Create the perturbed image by adjusting each pixel of the input image\r\n",
        "    perturbed_image = image + epsilon*sign_data_grad\r\n",
        "    # Adding clipping to maintain [0,1] range\r\n",
        "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\r\n",
        "    # Return the perturbed image\r\n",
        "    return perturbed_image\r\n",
        "\r\n",
        "def fgsm_attack2(image, epsilon, data_grad, mask):\r\n",
        "    \r\n",
        "    # Collect the element-wise sign of the data gradient\r\n",
        "    sign_data_grad = torch.mul(data_grad.sign(), mask.view(-1, 1, 1, 1))\r\n",
        "\r\n",
        "    # Create the perturbed image by adjusting each pixel of the input image\r\n",
        "    perturbed_image = image + epsilon*sign_data_grad\r\n",
        "    # Adding clipping to maintain [0,1] range\r\n",
        "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\r\n",
        "    # Return the perturbed image\r\n",
        "    return perturbed_image\r\n",
        "\r\n",
        "\r\n",
        "def generate_fgsm_attack(model, test_loader, epsilon):\r\n",
        "    '''\r\n",
        "    return: attack instance generated, and laebl \r\n",
        "    '''\r\n",
        "    \r\n",
        "    # Accuracy counter\r\n",
        "    correct = 0\r\n",
        "    adv_examples = []\r\n",
        "\r\n",
        "    # Loop over all examples in test set\r\n",
        "    for data, target in test_loader:\r\n",
        "\r\n",
        "        # Send the data and label to the device\r\n",
        "        data, target = data.to(device), target.to(device)\r\n",
        "\r\n",
        "        # Set requires_grad attribute of tensor. Important for Attack\r\n",
        "        data.requires_grad = True\r\n",
        "\r\n",
        "        # Forward pass the data through the model\r\n",
        "        output = model(data)\r\n",
        "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\r\n",
        "\r\n",
        "        # mask: 1 for correct, only update grad on correct image\r\n",
        "        mask = torch.eq(init_pred.flatten(), target.flatten()).float()\r\n",
        "\r\n",
        "        # Calculate the loss\r\n",
        "        loss = criterion(output, target)\r\n",
        "\r\n",
        "        # Zero all existing gradients\r\n",
        "        model.zero_grad()\r\n",
        "\r\n",
        "        # Calculate gradients of model in backward pass\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # Collect datagrad\r\n",
        "        data_grad = data.grad.data\r\n",
        "\r\n",
        "        # Call FGSM Attack\r\n",
        "        # perturbed_data = fgsm_attack2(data, epsilon, data_grad, mask)\r\n",
        "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\r\n",
        "\r\n",
        "        adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\r\n",
        "        adv_examples.append((target.flatten().detach().cpu().numpy(), adv_ex))\r\n",
        "        # pred_list.append((init_pred.flatten().detach().cpu().numpy(), final_pred.flatten().detach().cpu().numpy()))\r\n",
        "        \r\n",
        "    label = [j for i in adv_examples for j in i[0]]\r\n",
        "    adv_ex = [j for i in adv_examples for j in i[1]]\r\n",
        "\r\n",
        "    # Return the accuracy and an adversarial example\r\n",
        "    return adv_ex, label\r\n",
        "\r\n",
        "\r\n",
        "class AdvDataset(Dataset):\r\n",
        "    def __init__(self, data, label):\r\n",
        "        self.data = torch.Tensor(data)\r\n",
        "        self.label = torch.Tensor(label)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.data)\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return self.data[idx], self.label[idx]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIk7CJb9iM7p"
      },
      "source": [
        "adv_ex, label = generate_fgsm_attack(pretrained_model, testloader, 2/256)\r\n",
        "attackdataset = AdvDataset(adv_ex, label)\r\n",
        "attackloader = torch.utils.data.DataLoader(attackdataset, batch_size=1, shuffle=True, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR_jSPsWirZ2"
      },
      "source": [
        "## PGD \r\n",
        "------------------------\r\n",
        "eps: maximum distortion of adversarial example compared to original input\r\n",
        "\r\n",
        "eps_iter: step size for each attack iteration\r\n",
        "\r\n",
        "nb_iter: Number of attack iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtHLPhMRivQ1"
      },
      "source": [
        "\"\"\"lamda=2\"\"\"\r\n",
        "\r\n",
        "def pgd_attack(model, images, labels, eps=0.3, alpha=2/256, iters=5) :\r\n",
        "    images = images.to(device)\r\n",
        "    labels = labels.to(device)\r\n",
        "    # loss = nn.CrossEntropyLoss()\r\n",
        "        \r\n",
        "    ori_images = images.data\r\n",
        "        \r\n",
        "    for i in range(iters) :    \r\n",
        "        images.requires_grad = True\r\n",
        "        outputs = model(images)\r\n",
        "\r\n",
        "        model.zero_grad()\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        adv_images = images + alpha*images.grad.sign()\r\n",
        "        eta = torch.clamp(adv_images - ori_images, min=-eps, max=eps)\r\n",
        "        images = torch.clamp(ori_images + eta, min=0, max=1).detach_()\r\n",
        "            \r\n",
        "    return images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZZazL767hRl"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtDJuzPhLfEY"
      },
      "source": [
        "## others"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPLbZQi5Lgtr"
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu6qG812LucZ"
      },
      "source": [
        "def init_weights(m):\n",
        "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_normal_(m.weight.data)\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "# def ResNet50SAP():\n",
        "#     return ResNetSAP(BottleneckSAP, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aztOJFmMYzf"
      },
      "source": [
        "## Non SAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zrpU5m5Ml1H"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1uBMTUfMTX-"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6nXQKUFMenK"
      },
      "source": [
        "## SAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YRuSGvhZzrF"
      },
      "source": [
        "import random\n",
        "\n",
        "class RandomSAP(nn.Module):\n",
        "    \"\"\"\n",
        "    The original paper is https://arxiv.org/abs/1803.01442.\n",
        "    ----------\n",
        "    self.is_valid bool : if this flag is True, inject SAP.\n",
        "    \"\"\"\n",
        "    def __init__(self, is_valid=False):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        ratio float : ratio of pruning which can be larger than 1.0.\n",
        "        is_valid bool : if this flag is True, inject SAP.\n",
        "        \"\"\"\n",
        "        super(RandomSAP, self).__init__()\n",
        "        self.is_valid = is_valid\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        If self.training or not self.is_valid, just return inputs.\n",
        "        If self.is_valid apply SAP to inputs and return the result tensor.\n",
        "        Parameters\n",
        "        ----------\n",
        "        inputs torch.Tensor : input tensor whose shape is [b, c, h, w].\n",
        "        Returns\n",
        "        -------\n",
        "        outputs torch.Tensor : just return inputs or stochastically pruned inputs.\n",
        "        \"\"\"\n",
        "        if not self.is_valid:\n",
        "            return inputs\n",
        "        else:\n",
        "            b, c, h, w = inputs.shape   # (batchsize, channelsize, imagesize)\n",
        "            inputs_1d = inputs.reshape([b, c * h * w])  # [b, c * h * w]\n",
        "            # inputs_len = len(inputs_1d)  # = b\n",
        "            \n",
        "            # torch.manual_seed(10)\n",
        "            drop_prob = torch.rand(inputs_1d.shape, dtype=torch.float64)\n",
        "            drop_node = torch.where(drop_node>0.5, drop_node, 0.)\n",
        "            drop_node = torch.DoubleTensor(drop_node).float().cuda() \n",
        "            outputs = drop_node.reshape([b, c, h, w])  # [b, c, h, w]\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3NjyZ05RWC9"
      },
      "source": [
        "from torch.distributions.multinomial import Multinomial\n",
        "\n",
        "class SAP(nn.Module):\n",
        "    \"\"\"SimpleModel represents a nn.Module of Stochastic Activation Pruning.\n",
        "    The original paper is https://arxiv.org/abs/1803.01442.\n",
        "    Attributes\n",
        "    ----------\n",
        "    self.ratio float : ratio of pruning which can be larger than 1.0.\n",
        "    self.is_valid bool : if this flag is True, inject SAP.\n",
        "    \"\"\"\n",
        "    def __init__(self, ratio=1, is_valid=False):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        ratio float : ratio of pruning which can be larger than 1.0.\n",
        "        is_valid bool : if this flag is True, inject SAP.\n",
        "        \"\"\"\n",
        "        super(SAP, self).__init__()\n",
        "        self.ratio = ratio\n",
        "        self.is_valid = is_valid\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        If self.training or not self.is_valid, just return inputs.\n",
        "        If self.is_valid apply SAP to inputs and return the result tensor.\n",
        "        Parameters\n",
        "        ----------\n",
        "        inputs torch.Tensor : input tensor whose shape is [b, c, h, w].\n",
        "        Returns\n",
        "        -------\n",
        "        outputs torch.Tensor : just return inputs or stochastically pruned inputs.\n",
        "        \"\"\"\n",
        "        # print(\"SAP: \", self.is_valid)\n",
        "        # if self.training or not self.is_valid:\n",
        "        if not self.is_valid:\n",
        "            return inputs\n",
        "        else:\n",
        "            b, c, h, w = inputs.shape\n",
        "            inputs_1d = inputs.reshape([b, c * h * w])  # [b, c * h * w]\n",
        "            # print(inputs_1d)\n",
        "            outputs = torch.zeros_like(inputs_1d)  # outputs with 0 initilization\n",
        "           \n",
        "            inputs_1d_sum = torch.sum(torch.abs(inputs_1d), dim=-1, keepdim=True)\n",
        "            inputs_1d_prob = torch.abs(inputs_1d) / inputs_1d_sum\n",
        "            \n",
        "            # r: num_nodes\n",
        "            num_sample = int(c * h * w * self.ratio)  \n",
        "\n",
        "            # multinomial(total_count:int, probs:tensor, logits:tensor)\n",
        "            idx = Multinomial(num_sample, inputs_1d_prob).sample()\n",
        "\n",
        "            # if nonzero, keep; else, drop, let be zeroes\n",
        "            outputs[idx.nonzero(as_tuple=True)] = inputs_1d[idx.nonzero(as_tuple=True)]\n",
        "\n",
        "            # pdb.set_trace()\n",
        "            # scale up\n",
        "            outputs = outputs / (1 - (1-inputs_1d_prob)**num_sample + 1e-12)\n",
        "            outputs = outputs.reshape([b, c, h, w])  # [b, c, h, w]\n",
        "            # print(\"OUT: \", outputs)\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoBet9svTt_U"
      },
      "source": [
        "class BasicBlockSAP(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, is_valid=False):\n",
        "        super(BasicBlockSAP, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "        self.sap1 = SAP(is_valid=is_valid)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.sap1(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgpZmKcRMwCD"
      },
      "source": [
        "class ResNetSAP(nn.Module):\n",
        "    \"\"\"Model represents a model mainly used in experiments.\n",
        "    Attributes\n",
        "    ----------\n",
        "    self.num_classes int : number of classes of dataset.\n",
        "    self.layers nn.ModuleDict : ModuleDict of models.\n",
        "    \"\"\"\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNetSAP, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # self.sap = SAP(is_valid)\n",
        "        # self.sap1 = nn.Dropout(0.5)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, is_valid=True)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride, is_valid=False):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride, is_valid))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        # out = self.sap(out, batch_idx)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STAvhnJkM8R0"
      },
      "source": [
        "def init_weights(m):\n",
        "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_normal_(m.weight.data)\n",
        "        \n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "def ResNet18SAP():\n",
        "    return ResNetSAP(BasicBlockSAP, [2, 2, 2, 2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErvZeo4KIURN"
      },
      "source": [
        "# Free-Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyccGfvzIQ7L"
      },
      "source": [
        "def train(net, epoch, trainloader):\n",
        "    net.train()\n",
        "\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "        del inputs\n",
        "        del targets\n",
        "\n",
        "    acc = correct/total\n",
        "    avg_loss = train_loss/total\n",
        "    \n",
        "    return avg_loss, acc\n",
        "\n",
        "\n",
        "def test(net, testloader):\n",
        "    net.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            \n",
        "            torch.cuda.empty_cache()\n",
        "            del inputs\n",
        "            del targets\n",
        "            \n",
        "    acc = correct/total\n",
        "    avg_loss = test_loss/total\n",
        "    \n",
        "    return avg_loss, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wWQhMyzT1xK"
      },
      "source": [
        "# Adv-training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9glD6ts2Wwl"
      },
      "source": [
        "## FGSM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUewfkrsT1FW"
      },
      "source": [
        "def fgsm_train(net, epoch, trainloader, eps=2/255):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        inputs.requires_grad = True\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        data_grad = inputs.grad.data\n",
        "        perturbed_data = fgsm_attack(inputs, eps, data_grad)\n",
        "        new_outputs = net(perturbed_data)\n",
        "        new_loss = criterion(new_outputs, targets)\n",
        "        new_loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += new_loss.item()\n",
        "        _, new_predicted = new_outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += new_predicted.eq(targets).sum().item()\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "        del inputs\n",
        "        del targets\n",
        "\n",
        "        # progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        #              % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    acc = correct/total\n",
        "    avg_loss = train_loss/total\n",
        "    \n",
        "    return avg_loss, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WflW5wlP2ZMI"
      },
      "source": [
        "## PGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ0NVWF_3zGM"
      },
      "source": [
        "cifar10_mean = (0.4914, 0.4822, 0.4465)\r\n",
        "cifar10_std = (0.2471, 0.2435, 0.2616)\r\n",
        "\r\n",
        "mu = torch.tensor(cifar10_mean).view(3,1,1).cuda()\r\n",
        "std = torch.tensor(cifar10_std).view(3,1,1).cuda()\r\n",
        "\r\n",
        "upper_limit = ((1 - mu) / std)\r\n",
        "lower_limit = ((0 - mu) / std)\r\n",
        "\r\n",
        "epsilon = (2/256.) / std\r\n",
        "\r\n",
        "step_size = 2\r\n",
        "\r\n",
        "iters = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l-0_MBfT9u2"
      },
      "source": [
        "def clamp(X, lower_limit, upper_limit):\n",
        "    return torch.max(torch.min(X, upper_limit), lower_limit)\n",
        "\n",
        "# Training\n",
        "def pgd_train(model, epoch, epsilon):\n",
        "    # start_train_time = time.time()\n",
        "\n",
        "    # logger.info('Epoch \\t Seconds \\t LR \\t \\t Train Loss \\t Train Acc')\n",
        "\n",
        "# for epoch in range(args.epochs):\n",
        "# for epoch in range(start_epoch, start_epoch+20):\n",
        "    start_epoch_time = time.time()\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    train_n = 0\n",
        "\n",
        "    for i, (X, y) in enumerate(trainloader):\n",
        "        X, y = X.cuda(), y.cuda()\n",
        "        delta = torch.zeros_like(X).cuda()\n",
        "\n",
        "        # if args.delta_init == 'random':\n",
        "        for i in range(len(epsilon)):\n",
        "            delta[:, i, :, :].uniform_(-epsilon[i][0][0].item(), epsilon[i][0][0].item())\n",
        "        delta.data = clamp(delta, lower_limit-X, upper_limit-X)\n",
        "\n",
        "        delta.requires_grad = True\n",
        "        for _ in range(iters):\n",
        "            output = model(X + delta)\n",
        "            loss = criterion(output, y)\n",
        "\n",
        "            # with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "            loss.backward()\n",
        "            \n",
        "            grad = delta.grad.detach()\n",
        "            delta.data = clamp(delta + step_size*torch.sign(grad), -epsilon, epsilon)\n",
        "            delta.data = clamp(delta, lower_limit-X, upper_limit-X)\n",
        "            delta.grad.zero_()\n",
        "        \n",
        "        delta = delta.detach()\n",
        "        output = model(X + delta)\n",
        "        loss = criterion(output, y)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        # with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item() * y.size(0)\n",
        "        train_acc += (output.max(1)[1] == y).sum().item()\n",
        "        train_n += y.size(0)\n",
        "        \n",
        "        # scheduler.step()\n",
        "    epoch_time = time.time()\n",
        "    # lr = scheduler.get_lr()[0]\n",
        "    # logger.info('%d \\t %.1f \\t \\t %.4f \\t %.4f \\t %.4f',\n",
        "    #             epoch, epoch_time-start_epoch_time, train_loss/train_n, train_acc/train_n)\n",
        "    \n",
        "    print(epoch_time-start_epoch_time)\n",
        "    return train_loss/train_n, train_acc/train_n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWjgIzAPEoei"
      },
      "source": [
        "#Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH5BTHqZEvFR"
      },
      "source": [
        "\"\"\"batch=1\"\"\"\n",
        "\n",
        "def get_matrix(net, testloader):\n",
        "    net.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    matrix = np.zeros((10,10))\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            \n",
        "            matrix[targets.detach().cpu().numpy(),predicted.detach().cpu().numpy()] += 1\n",
        "\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            del inputs\n",
        "            del targets\n",
        "            \n",
        "    acc = correct/total\n",
        "    avg_loss = test_loss/total\n",
        "    \n",
        "    return avg_loss, acc, matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzru2YVef-Op"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "%cd /content/gdrive/\"My Drive\"/GameTheory/\n",
        "\n",
        "def write_matrix(matrix, filename):\n",
        "    with open(filename, \"w\", newline='') as f:\n",
        "        file = csv.writer(f, delimiter=',')\n",
        "        file.writerow([\"target\", \"predict\"])\n",
        "        for i, row in enumerate(matrix):\n",
        "            file.writerow([i, row])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-1CNd_0HN-X"
      },
      "source": [
        "get_matrix(pretrained_model, attack_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_zHuxBIKmB8"
      },
      "source": [
        "get_matrix(confusemodel, attack_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_Ji00PXLNMb"
      },
      "source": [
        "import gc\n",
        "\n",
        "del confusemodel\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqkIHdSmK4xy"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "modelpath = path + \"ResNet18_19.pth\"\n",
        "# /content/gdrive/MyDrive/GameTheory/Adc_ResNet50_19.pth\n",
        "confusemodel = ResNet18().to(device)\n",
        "confusemodel.load_state_dict(torch.load(modelpath)['model_state_dict'])\n",
        "pretrained_dict = confusemodel.state_dict()\n",
        "# confusemodel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLeqdJpGzHbx"
      },
      "source": [
        "loss,acc,m = get_matrix(confusemodel, attack_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u-CKWuMf8WV"
      },
      "source": [
        "write_matrix(m, \"ResNet18_20epoch.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wic9pXEboiqx"
      },
      "source": [
        "# Gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLGr9fheoG_m"
      },
      "source": [
        "def get_grad(model, test_loader):\n",
        "    '''\n",
        "    return: Gradient of image, num_imgs x channel x w x h,\n",
        "    10000 x 3 x 32 x 32\n",
        "    '''\n",
        "    grad_list = []\n",
        "    # Loop over all examples in test set\n",
        "    for data, target in test_loader:\n",
        "\n",
        "        # Send the data and label to the device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Set requires_grad attribute of tensor. Important for Attack\n",
        "        data.requires_grad = True\n",
        "\n",
        "        # Forward pass the data through the model\n",
        "        output = model(data)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Zero all existing gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Calculate gradients of model in backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Collect datagrad\n",
        "        data_grad = data.grad.data\n",
        "\n",
        "        grad_list.append(data_grad.detach().cpu().numpy())\n",
        "\n",
        "        del data, target, data_grad, output\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    return np.concatenate(grad_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xoOG51NjH8W"
      },
      "source": [
        "# Attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6QnldNpktw2"
      },
      "source": [
        "## Grey-box"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q6smiNnkxIS"
      },
      "source": [
        "def grey_box_attack(model, attack_loader):\r\n",
        "    '''\r\n",
        "    attack_loader: dataloader of attack instance generated by resnet18 free-trained model\r\n",
        "    '''\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    correct = 0.\r\n",
        "    for data, target in attack_loader:\r\n",
        "\r\n",
        "        # Send the data and label to the device\r\n",
        "        data, target = data.to(device), target.to(device)\r\n",
        "\r\n",
        "        # Forward pass the data through the model\r\n",
        "        output = model(data)\r\n",
        "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\r\n",
        "        # print(\"predict: \", pred.detach().cpu().numpy(), target.detach().cpu().numpy())\r\n",
        "        # Zero all existing gradients\r\n",
        "        model.zero_grad()\r\n",
        "\r\n",
        "        # calculate correct prediction\r\n",
        "        correct += torch.sum(torch.eq(pred.flatten(), target.flatten())).item()\r\n",
        "        # Special case for saving 0 epsilon examples\r\n",
        "\r\n",
        "    # Calculate final accuracy for this epsilon\r\n",
        "    final_acc = correct/float(len(attack_loader))\r\n",
        "\r\n",
        "    # print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(testloader), acc))\r\n",
        "    # Return the accuracy and an adversarial example\r\n",
        "    return final_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7nAu7ETBMlL"
      },
      "source": [
        "## Non SAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xelOTRJKFXPx"
      },
      "source": [
        "\"\"\"\n",
        "perturbed_data = fgsm_attack(inputs, epsilon, data_grad)\n",
        "\"\"\"\n",
        "\n",
        "def testattack(net, testloader, epsilon):\n",
        "    # global best_acc\n",
        "    net.eval()\n",
        "\n",
        "    # test_loss = 0\n",
        "    correct = 0\n",
        "    # total = 0\n",
        "    # with torch.no_grad():\n",
        "    for b, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # inputs, targets = Variable(inputs), Variable(targets)-1\n",
        "        \n",
        "        inputs.requires_grad = True\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        # if predicted.item() != targets.item():\n",
        "        #     continue\n",
        "        loss = criterion(outputs, targets)\n",
        "        net.zero_grad()\n",
        "\n",
        "        # test_loss += loss.item()\n",
        "        loss.backward()\n",
        "        data_grad = inputs.grad.data\n",
        "        perturbed_data = fgsm_attack(inputs, epsilon, data_grad)\n",
        "        new_outputs = net(perturbed_data)\n",
        "        _, new_predicted = new_outputs.max(1)\n",
        "\n",
        "        # total += targets.size(0)\n",
        "        correct += new_predicted.eq(targets).sum().item()\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        del inputs\n",
        "        del targets\n",
        "        \n",
        "    acc = correct/float(len(testloader))\n",
        "    # avg_loss = test_loss/total\n",
        "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(testloader), acc))\n",
        "\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQdw25t61c3Z"
      },
      "source": [
        "## SAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_o1eTO7mc5r"
      },
      "source": [
        "import pdb\n",
        "import time\n",
        "def testattackSAP(net, net_sap, testloader, epsilon):\n",
        "    # global best_acc\n",
        "    net.eval()\n",
        "\n",
        "    # test_loss = 0\n",
        "    correct = 0.\n",
        "    correct_sap = 0.\n",
        "    # total = 0\n",
        "    # with torch.no_grad():\n",
        "    nonsap = []\n",
        "    nonsap_att = []\n",
        "    sap = []\n",
        "    sap_att = []\n",
        "    ytrue = []\n",
        "    \n",
        "    start = time.time()\n",
        "    for b, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # inputs, targets = Variable(inputs), Variable(targets)-1\n",
        "        \n",
        "        inputs.requires_grad = True\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        outputs_sap = net_sap(inputs)\n",
        "        # nonsap.append(outputs.detach().cpu().numpy())\n",
        "        # sap.append(outputs_sap.detach().cpu().numpy())\n",
        "        # print(\"[non sap]: \",outputs, \"\\n[sap]: \", outputs_sap, \"\\n\")\n",
        "        \n",
        "        _, predicted = outputs.max(1)\n",
        "        _, predicted_sap = outputs_sap.max(1)\n",
        "        # if predicted.item() != targets.item():\n",
        "        #     continue\n",
        "        loss = criterion(outputs, targets)\n",
        "        # loss_sap = criterion(outputs_sap, targets)\n",
        "\n",
        "        net.zero_grad()\n",
        "        # net_sap.zero_grad()\n",
        "\n",
        "        # test_loss += loss.item()\n",
        "        loss.backward()\n",
        "        # loss_sap.backward()\n",
        "\n",
        "        # pdb.set_trace()\n",
        "        data_grad = inputs.grad.data\n",
        "        perturbed_data = fgsm_attack(inputs, epsilon, data_grad)\n",
        "        \n",
        "        new_outputs = net(perturbed_data)\n",
        "        _, new_predicted = new_outputs.max(1)\n",
        "        new_outputs_sap = net_sap(perturbed_data)\n",
        "        _, new_predicted_sap = new_outputs_sap.max(1)\n",
        "\n",
        "        # print(\"nonsap: \", new_predicted)\n",
        "        # print(\"sap: \", new_predicted_sap, \"\\n\")\n",
        "        nonsap.append(predicted.detach().cpu().numpy())\n",
        "        nonsap_att.append(new_predicted.detach().cpu().numpy())\n",
        "        sap.append(predicted_sap.detach().cpu().numpy())\n",
        "        sap_att.append(new_predicted_sap.detach().cpu().numpy())\n",
        "        ytrue.append(targets.detach().cpu().numpy())\n",
        "        # total += targets.size(0)\n",
        "        # correct += torch.sum(torch.eq(pred.flatten(), targets.flatten())).item()\n",
        "        correct += new_predicted.eq(targets).sum().item()\n",
        "        correct_sap += new_predicted_sap.eq(targets).sum().item()\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        del inputs\n",
        "        del targets\n",
        "        \n",
        "    acc = correct / float(len(testloader))\n",
        "    acc_sap = correct_sap / float(len(testloader))\n",
        "    # avg_loss = test_loss/total\n",
        "    print(\"Time: \", time.time()-start)\n",
        "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(testloader), acc))\n",
        "    print(\"Epsilon: {}\\tSAP Test Accuracy = {} / {} = {}\".format(epsilon, correct_sap, len(testloader), acc_sap))\n",
        "\n",
        "    return acc,acc_sap,nonsap,nonsap_att,sap,sap_att,ytrue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpqYktwElzKo"
      },
      "source": [
        "## Multi-SAP\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_vt0LVDlnx-"
      },
      "source": [
        "def testattackMultiSAP(net, net_sap, testloader, epsilon):\n",
        "    # global best_acc\n",
        "    net.eval()\n",
        "\n",
        "    # test_loss = 0\n",
        "    # correct = 0.\n",
        "    correct_sap = 0.\n",
        "    # total = 0\n",
        "    # with torch.no_grad():\n",
        "    # nonsap = []\n",
        "    sap = []\n",
        "    ytrue = []\n",
        "    \n",
        "    start = time.time()\n",
        "    for b, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # inputs, targets = Variable(inputs), Variable(targets)-1\n",
        "        \n",
        "        inputs.requires_grad = True\n",
        "\n",
        "        # outputs = net(inputs)\n",
        "\n",
        "        for i in range(10):\n",
        "            outputs_sap = net_sap(inputs)\n",
        "        # nonsap.append(outputs.detach().cpu().numpy())\n",
        "        # sap.append(outputs_sap.detach().cpu().numpy())\n",
        "        # print(\"[non sap]: \",outputs, \"\\n[sap]: \", outputs_sap, \"\\n\")\n",
        "        \n",
        "        # _, predicted = outputs.max(1)\n",
        "        _, predicted_sap = outputs_sap.max(1)\n",
        "        # if predicted.item() != targets.item():\n",
        "        #     continue\n",
        "        # loss = criterion(outputs, targets)\n",
        "        loss_sap = criterion(outputs_sap, targets)\n",
        "\n",
        "        # net.zero_grad()\n",
        "        net_sap.zero_grad()\n",
        "\n",
        "        # test_loss += loss.item()\n",
        "        # loss.backward()\n",
        "        loss_sap.backward()\n",
        "\n",
        "        # pdb.set_trace()\n",
        "        data_grad = inputs.grad.data\n",
        "        perturbed_data = fgsm_attack(inputs, epsilon, data_grad)\n",
        "        \n",
        "        # new_outputs = net(perturbed_data)\n",
        "        # _, new_predicted = new_outputs.max(1)\n",
        "        new_outputs_sap = net_sap(perturbed_data)\n",
        "        _, new_predicted_sap = new_outputs_sap.max(1)\n",
        "\n",
        "        # print(\"nonsap: \", new_predicted)\n",
        "        # print(\"sap: \", new_predicted_sap, \"\\n\")\n",
        "        # nonsap.append(new_predicted.detach().cpu().numpy())\n",
        "        sap.append(new_predicted_sap.detach().cpu().numpy())\n",
        "        ytrue.append(targets.detach().cpu().numpy())\n",
        "        # total += targets.size(0)\n",
        "        # correct += torch.sum(torch.eq(pred.flatten(), targets.flatten())).item()\n",
        "        correct += new_predicted.eq(targets).sum().item()\n",
        "        correct_sap += new_predicted_sap.eq(targets).sum().item()\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        del inputs\n",
        "        del targets\n",
        "        \n",
        "    acc = correct / float(len(testloader))\n",
        "    acc_sap = correct_sap / float(len(testloader))\n",
        "    # avg_loss = test_loss/total\n",
        "    print(\"Time: \", time.time()-start)\n",
        "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(testloader), acc))\n",
        "    print(\"Epsilon: {}\\tSAP Test Accuracy = {} / {} = {}\".format(epsilon, correct_sap, len(testloader), acc_sap))\n",
        "\n",
        "    return acc,nonsap,sap,ytrue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fITSiBlAIgAM"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H859vlvbcvaq"
      },
      "source": [
        "import gc\n",
        "\n",
        "del resmodel\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph6w1GiYYiLZ"
      },
      "source": [
        "\"\"\"ResNet18 Model\"\"\"\n",
        "resmodel = ResNet18().to(device)\n",
        "resmodel.apply(init_weights)\n",
        "\n",
        "if device == 'cuda':\n",
        "    resmodel = torch.nn.DataParallel(resmodel)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(resmodel.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hokvaXRjgXpj"
      },
      "source": [
        "## FGSM train\n",
        "----------------------\n",
        "lamda=2, epsilon=2/256"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydkAGKPzge1C"
      },
      "source": [
        "for epoch in range(start_epoch, start_epoch+30):\n",
        "    if epoch > 0:\n",
        "        train_loss, train_acc = train(resmodel, epoch, trainloader)\n",
        "        adv_train_loss, adv_train_acc = fgsm_train(resmodel, epoch, trainloader)\n",
        "    val_loss, val_acc = test(resmodel, testloader)\n",
        "    \n",
        "    print('[Epoch: {}]\\nTrain Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tAdv_Train Loss: {:.4f}\\tAdv_Train Accuracy: {:.4f}\\nVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
        "          format(epoch, train_loss, train_acc, adv_train_loss, adv_train_acc, val_loss, val_acc))\n",
        "    \n",
        "    if (epoch+1)%10 == 0:\n",
        "        torch.save({'model_state_dict': resmodel.state_dict(),},\n",
        "                    path + \"Adv_ResNet18_lamda2_{}.pth\".format(str(epoch+1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBcY6TMLFeek"
      },
      "source": [
        "def test_advtrain_ave(model, loader, num):\n",
        "    start = time.time()\n",
        "    res = 0\n",
        "    for _ in range(num):\n",
        "        acc = test(model, loader)[1]\n",
        "        res += acc\n",
        "    print(\"Time: \", (time.time()-start)/60, \" min\")\n",
        "    return res/num"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6wK_AUnFgOr"
      },
      "source": [
        "test_advtrain_ave(adv_SAPmodel, testloader, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46oP_Tgl25PD"
      },
      "source": [
        "## PGD train\r\n",
        "-----------------\r\n",
        "lamda=2, epsilon=2/256"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK-4M6VV278R"
      },
      "source": [
        "for epoch in range(start_epoch, start_epoch+30):\r\n",
        "    if epoch > 0:\r\n",
        "        train_loss, train_acc = train(resmodel, epoch, trainloader)\r\n",
        "        adv_train_loss, adv_train_acc = fgsm_train(resmodel, epoch, trainloader)\r\n",
        "    val_loss, val_acc = test(resmodel, testloader)\r\n",
        "    \r\n",
        "    print('[Epoch: {}]\\nTrain Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tAdv_Train Loss: {:.4f}\\tAdv_Train Accuracy: {:.4f}\\nVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\r\n",
        "          format(epoch, train_loss, train_acc, adv_train_loss, adv_train_acc, val_loss, val_acc))\r\n",
        "    \r\n",
        "    if (epoch+1)%10 == 0:\r\n",
        "        torch.save({'model_state_dict': resmodel.state_dict(),},\r\n",
        "                    path + \"Adv_ResNet18_lamda2_{}.pth\".format(str(epoch+1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-UXfgypT8zI"
      },
      "source": [
        "## free train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5H7Fw_3eS_T"
      },
      "source": [
        "\"\"\"res18\"\"\"\n",
        "for epoch in range(start_epoch, start_epoch+20):\n",
        "    train_loss, train_acc = train(resmodel, epoch, trainloader)\n",
        "    val_loss, val_acc = test(resmodel, testloader)\n",
        "    \n",
        "    print('[Epoch: {}]\\nTrain Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
        "          format(epoch, train_loss, train_acc, val_loss, val_acc))\n",
        "    \n",
        "    # if (epoch+1)%10 == 0:\n",
        "torch.save({'model_state_dict': resmodel.state_dict(),},\n",
        "            path + \"ResNet18_{}.pth\".format(str(epoch)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC1IAa95KpcX"
      },
      "source": [
        "# Reload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1gZ1SywxyPQ"
      },
      "source": [
        "## adv train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKGahaG_x247"
      },
      "source": [
        "### Non SAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dclpzc9tx3o9"
      },
      "source": [
        "# epsilons = 2/255\n",
        "\n",
        "\"\"\"Load Model\"\"\"\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "modelpath = path + \"Adv_ResNet18_lamda2_10.pth\"\n",
        "\n",
        "pretrained_adv_model = ResNet18().to(device)\n",
        "pretrained_adv_model.load_state_dict(torch.load(modelpath)['model_state_dict'])\n",
        "pretrained_adv_dict = pretrained_adv_model.state_dict()\n",
        "# pretrained_adv_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKHWil8bI0lj"
      },
      "source": [
        "grey_box_attack(pretrained_adv_model, attackloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5A8muTIyba1"
      },
      "source": [
        "### SAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q52jn1mkyfay"
      },
      "source": [
        "del adv_SAPmodel\n",
        "# del model_adv_dict\n",
        "\n",
        "adv_SAPmodel = ResNet18SAP().to(device)\n",
        "model_adv_dict = adv_SAPmodel.state_dict()\n",
        "pretrained_adv_dict = {k: v for k, v in pretrained_adv_dict.items() if k in model_adv_dict}\n",
        "model_adv_dict.update(pretrained_adv_dict)\n",
        "adv_SAPmodel.load_state_dict(pretrained_adv_dict)\n",
        "# adv_SAPmodel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahHl-QkcmdWy"
      },
      "source": [
        "grey_box_attack(adv_SAPmodel, attackloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQHoVq1g6kCI"
      },
      "source": [
        "acc,acc_sap,nonsap,nonsap_att,sap,sap_att,ytrue = testattackSAP(pretrained_adv_model, adv_SAPmodel, attackloader, epsilons)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrvZWxsR-JVl"
      },
      "source": [
        "x = np.linspace(0,200,200)\n",
        "plt.plot(x,sap[:200],'b.',label=\"SAP\")\n",
        "plt.plot(x,ytrue[:200],'r.',label=\"true\")\n",
        "plt.legend() \n",
        "plt.title(\"Predictions before Attack\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(x,sap_att[:200],'b.', label=\"SAP_att\")\n",
        "plt.plot(x,ytrue[:200],'r.',label=\"true\")\n",
        "plt.legend() \n",
        "plt.title(\"Predictions after Attack\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Escct0ACxuCX"
      },
      "source": [
        "## free train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSuwOA4_CVCx"
      },
      "source": [
        "### Non SAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWNeoUuUcIwG"
      },
      "source": [
        "# epsilons = 0.05\n",
        "del pretrained_model\n",
        "del pretrained_dict\n",
        "\n",
        "\"\"\"Load Model\"\"\"\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "modelpath = path + \"ResNet18_19.pth\"\n",
        "\n",
        "pretrained_model = ResNet18().to(device)\n",
        "pretrained_model.load_state_dict(torch.load(modelpath)['model_state_dict'])\n",
        "pretrained_dict = pretrained_model.state_dict()\n",
        "# pretrained_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHZbRTa4lw96"
      },
      "source": [
        "testattack(pretrained_model, attackloader, epsilons)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc_QHUcMk8xS"
      },
      "source": [
        "grey_box_attack(pretrained_model, attackloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZlRMDqgmnT1"
      },
      "source": [
        "### Multi-SAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlQG0U9NvGVw"
      },
      "source": [
        "random_SAPmodel = ResNet18SAP().to(device)\n",
        "model_dict = random_SAPmodel.state_dict()\n",
        "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "model_dict.update(pretrained_dict)\n",
        "random_SAPmodel.load_state_dict(pretrained_dict)\n",
        "random_SAPmodel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqNqlRnr9GH0"
      },
      "source": [
        "def get_batch_grad(epoch, model, loader):\n",
        "    for e in range(epoch):\n",
        "        grad = get_grad(model, loader)\n",
        "        np.save(\"sap_{}_grads.npy\".format(e), grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_nVXF269s6q"
      },
      "source": [
        "get_batch_grad(10, random_SAPmodel, attack_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjH2KZur_8qs"
      },
      "source": [
        "!mv sap_0_grads.npy \"/content/gdrive/My Drive/GameTheory/cal_grad/\"\n",
        "!mv sap_1_grads.npy \"/content/gdrive/My Drive/GameTheory/cal_grad/\"\n",
        "!mv sap_2_grads.npy \"/content/gdrive/My Drive/GameTheory/cal_grad/\"\n",
        "!mv sap_3_grads.npy \"/content/gdrive/My Drive/GameTheory/cal_grad/\"\n",
        "!mv sap_4_grads.npy \"/content/gdrive/My Drive/GameTheory/cal_grad/\"\n",
        "!mv sap_5_grads.npy \"/content/gdrive/My Drive/GameTheory/cal_grad/\"\n",
        "!mv sap_6_grads.npy \"/content/gdrive/My Drive/GameTheory/cal_grad/\"\n",
        "!mv sap_7_grads.npy \"/content/gdrive/My Drive/GameTheory/cal_grad/\"\n",
        "!mv sap_8_grads.npy \"/content/gdrive/My Drive/GameTheory/cal_grad/\"\n",
        "!mv sap_9_grads.npy \"/content/gdrive/My Drive/GameTheory/cal_grad/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK4Sltv9lbDN"
      },
      "source": [
        "acc,acc_sap,nonsap,nonsap_att,sap,sap_att,ytrue = testattackSAP(pretrained_model, random_SAPmodel, attack_loader, 0.05)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NwLB1O5hiEt"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# x = [1,2,3,4,5,6,7,8,9,10]\n",
        "x = np.linspace(0,100,100)\n",
        "plt.plot(x,sap[:100],'b.',label=\"SAP\")\n",
        "plt.plot(x,ytrue[:100],'r.',label=\"true\")\n",
        "plt.legend() \n",
        "plt.title(\"Predictions before Attack\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(x,sap_att[:100],'b.', label=\"SAP_att\")\n",
        "plt.plot(x,ytrue[:100],'r.',label=\"true\")\n",
        "plt.legend() \n",
        "plt.title(\"Predictions after Attack\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhPMAXZEh1qE"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# x = [1,2,3,4,5,6,7,8,9,10]\n",
        "x = np.linspace(0,100,100)\n",
        "plt.plot(x,nonsap[:100],'b.',label=\"nonSAP\")\n",
        "plt.plot(x,ytrue[:100],'r.',label=\"true\")\n",
        "plt.legend() \n",
        "plt.title(\"Predictions before Attack\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(x,nonsap_att[:100],'b.', label=\"nonSAP_att\")\n",
        "plt.plot(x,ytrue[:100],'r.',label=\"true\")\n",
        "plt.legend() \n",
        "plt.title(\"Predictions after Attack\")\n",
        "plt.show()\n",
        "\n",
        "# plt.plot(x,sap_att[:100],'b.', label=\"SAP_att\")\n",
        "# plt.plot(ytrue[:100],sap_att[:100],'r.',label=\"true\")\n",
        "# plt.legend() \n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2SkSlAdtvuk"
      },
      "source": [
        "all_grad = []\n",
        "all_pred = []\n",
        "for inputs, targets in attackloader:\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    \n",
        "    inputs.requires_grad = True\n",
        "\n",
        "    # outputs = net(inputs)\n",
        "    grad_list = []\n",
        "    pred_list = []\n",
        "\n",
        "    for i in range(10):\n",
        "        start = time.time()\n",
        "\n",
        "        outputs_sap = random_SAPmodel(inputs)\n",
        "    # nonsap.append(outputs.detach().cpu().numpy())\n",
        "    # sap.append(outputs_sap.detach().cpu().numpy())\n",
        "    # print(\"[non sap]: \",outputs, \"\\n[sap]: \", outputs_sap, \"\\n\")\n",
        "    \n",
        "    # _, predicted = outputs.max(1)\n",
        "        _, predicted_sap = outputs_sap.max(1)\n",
        "        pred_list.append(predicted_sap.detach().cpu().numpy())\n",
        "    # if predicted.item() != targets.item():\n",
        "    #     continue\n",
        "    # loss = criterion(outputs, targets)\n",
        "        loss_sap = criterion(outputs_sap, targets)\n",
        "\n",
        "    # net.zero_grad()\n",
        "        random_SAPmodel.zero_grad()\n",
        "\n",
        "    # test_loss += loss.item()\n",
        "    # loss.backward()\n",
        "        loss_sap.backward()\n",
        "\n",
        "    # pdb.set_trace()\n",
        "        data_grad = inputs.grad.data\n",
        "\n",
        "        grad_list.append(data_grad.detach().cpu().numpy())\n",
        "        \n",
        "        # print(\"Time: \", time.time()-start, data_grad.detach().cpu().numpy())\n",
        "\n",
        "    grad_list = np.concatenate(grad_list)\n",
        "    pred_list = np.concatenate(pred_list)\n",
        "\n",
        "    print(\"Target: {}\\nPredictions: {}\".format(targets.detach().cpu().numpy(), pred_list))\n",
        "\n",
        "    all_grad.append(grad_list)\n",
        "    all_pred.append(pred_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8paFPPd-AX6"
      },
      "source": [
        "### SAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd-Rd37DC4jU"
      },
      "source": [
        "import gc\n",
        "\n",
        "# del pretrained_model\n",
        "del SAPmodel\n",
        "del model_dict\n",
        "# del adv_SAPmodel\n",
        "# del random_SAPmodel\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NN0snOBqFqV"
      },
      "source": [
        "SAPmodel = ResNet18SAP().to(device)\n",
        "model_dict = SAPmodel.state_dict()\n",
        "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "model_dict.update(pretrained_dict)\n",
        "SAPmodel.load_state_dict(pretrained_dict)\n",
        "# SAPmodel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stYCIahHl2N0"
      },
      "source": [
        "acc,nonsap,sap = testattackSAP(pretrained_model, SAPmodel, attackloader, epsilons)\r\n",
        "ns = np.squeeze(np.array(nonsap))\r\n",
        "s = np.squeeze(np.array(sap))\r\n",
        "\r\n",
        "ns_T = ns.T\r\n",
        "s_T = s.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q78nBnihA45"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = [1,2,3,4,5,6,7,8,9,10]\n",
        "plt.plot(x,ns[0],label=\"nonSAP\")\n",
        "plt.plot(x,s[0],label=\"SAP\")\n",
        "plt.legend() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d26KwSZRrRg9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = [1,2,3,4,5,6,7,8,9,10]\n",
        "plt.plot(x,ns[1],label=\"nonSAP\")\n",
        "plt.plot(x,s[1],label=\"SAP\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_HkkW--ok4s"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "x = np.linspace(0,10000,10000)\n",
        "\n",
        "for i in range(len(classes)):\n",
        "    plt.plot(x,ns_T[i],label=\"nonSAP\")\n",
        "    plt.plot(x,s_T[i],label=\"SAP\")\n",
        "    plt.legend()\n",
        "    plt.title(classes[i])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}